"""
ä¸»è®­ç»ƒç®¡é“ - åè°ƒæ•°æ®è·å–ã€ç‰¹å¾å·¥ç¨‹ã€HMM å’Œ LSTM è®­ç»ƒ

ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜ï¼š
1. å…ˆæŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®ä¸º train/val/test
2. HMM åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼ˆscaler, PCA, HMM å‚æ•°ï¼‰
3. ç”¨è®­ç»ƒå¥½çš„ HMM å¯¹éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼ˆæ— æ•°æ®æ³„æ¼ï¼‰
4. LSTM ä½¿ç”¨ç‹¬ç«‹çš„éªŒè¯é›†å’Œæµ‹è¯•é›†
"""
import logging
import os
from datetime import datetime
from typing import Dict, Tuple
import pandas as pd
import numpy as np

from config import TrainingConfig, setup_logging
from data_fetcher import BinanceDataFetcher
from feature_engineering import FeatureEngineer
from hmm_trainer import HMMRegimeLabeler
from lstm_trainer import LSTMRegimeClassifier
from model_registry import allocate_version_id, register_version
from forward_testing import on_training_finished as forward_test_on_training_finished, ForwardTestCronManager

setup_logging(log_file='training.log', level=logging.INFO)
logger = logging.getLogger(__name__)

class TrainingPipeline:
    """è®­ç»ƒç®¡é“"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.data_fetcher = BinanceDataFetcher(
            api_key=config.BINANCE_API_KEY,
            api_secret=config.BINANCE_API_SECRET
        )
        self.feature_engineer = FeatureEngineer(cache_manager=self.data_fetcher.cache_manager)
    
    def _split_data_by_time(
        self, 
        features: pd.DataFrame, 
        train_ratio: float = 0.7,
        val_ratio: float = 0.15,
        test_ratio: float = 0.15
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®ä¸º train/val/test
        
        Args:
            features: å®Œæ•´ç‰¹å¾ DataFrame
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            (train_features, val_features, test_features)
        """
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
            "train_ratio + val_ratio + test_ratio å¿…é¡»ç­‰äº 1.0"
        
        n = len(features)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        train_features = features.iloc[:train_end]
        val_features = features.iloc[train_end:val_end]
        test_features = features.iloc[val_end:]
        
        logger.info(f"æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†:")
        logger.info(f"  è®­ç»ƒé›†: {len(train_features)} è¡Œ ({train_ratio:.0%})")
        logger.info(f"  éªŒè¯é›†: {len(val_features)} è¡Œ ({val_ratio:.0%})")
        logger.info(f"  æµ‹è¯•é›†: {len(test_features)} è¡Œ ({test_ratio:.0%})")
        
        if len(train_features) > 0 and len(test_features) > 0:
            logger.info(f"  è®­ç»ƒé›†æ—¶é—´èŒƒå›´: {train_features.index.min()} ~ {train_features.index.max()}")
            logger.info(f"  éªŒè¯é›†æ—¶é—´èŒƒå›´: {val_features.index.min()} ~ {val_features.index.max()}")
            logger.info(f"  æµ‹è¯•é›†æ—¶é—´èŒƒå›´: {test_features.index.min()} ~ {test_features.index.max()}")
        
        return train_features, val_features, test_features
    
    def _log_state_adjustment_summary(
        self, 
        original_n_states: int, 
        new_n_states: int, 
        new_mapping: Dict[int, str]
    ):
        """
        è¾“å‡ºçŠ¶æ€è°ƒæ•´çš„è¯¦ç»†æ€»ç»“
        
        Args:
            original_n_states: åŸå§‹çŠ¶æ€æ•°é‡
            new_n_states: è°ƒæ•´åçš„çŠ¶æ€æ•°é‡
            new_mapping: æ–°çš„çŠ¶æ€æ˜ å°„
        """
        # å®Œæ•´çš„ 6 ä¸ªè¯­ä¹‰åç§°
        all_regime_names = {
            "Strong_Trend", "Weak_Trend", "Range", 
            "Choppy_High_Vol", "Volatility_Spike", "Squeeze"
        }
        
        # å½“å‰ä¿ç•™çš„è¯­ä¹‰åç§°
        current_names = set(new_mapping.values())
        
        # è¢«åˆ é™¤çš„è¯­ä¹‰åç§°
        removed_names = all_regime_names - current_names
        
        logger.info("=" * 70)
        logger.info("ğŸ“Š çŠ¶æ€æ•°é‡è°ƒæ•´æ€»ç»“")
        logger.info("=" * 70)
        logger.info(f"  åŸå§‹çŠ¶æ€æ•°é‡: {original_n_states}")
        logger.info(f"  è°ƒæ•´åæ•°é‡:   {new_n_states}")
        logger.info(f"  ")
        logger.info(f"  âœ… ä¿ç•™çš„çŠ¶æ€ ({len(current_names)} ä¸ª):")
        for name in sorted(current_names):
            logger.info(f"     - {name}")
        
        if removed_names:
            logger.info(f"  ")
            logger.info(f"  âŒ åˆ é™¤çš„çŠ¶æ€ ({len(removed_names)} ä¸ª):")
            for name in sorted(removed_names):
                logger.info(f"     - {name} (è¯¥å¸‚åœºçŠ¶æ€åœ¨éªŒè¯/æµ‹è¯•æœŸæœªå‡ºç°)")
        
        logger.info("=" * 70)
    
    def full_retrain(self, symbol: str, primary_timeframe: str = None, version_id: str = None) -> Dict:
        """
        å®Œæ•´é‡è®­ï¼ˆä»é›¶å¼€å§‹ï¼‰
        
        ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜ï¼š
        1. å…ˆæŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®ä¸º train/val/test
        2. HMM åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
        3. LSTM ä½¿ç”¨ç‹¬ç«‹çš„éªŒè¯é›†å’Œæµ‹è¯•é›†
        
        Args:
            symbol: äº¤æ˜“å¯¹
            primary_timeframe: ä¸»æ—¶é—´æ¡†æ¶ï¼ˆå¦‚ "5m", "15m" æˆ– "1h"ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            version_id: ç‰ˆæœ¬ç›®å½• idï¼ˆå¦‚ 2025-01-31-1ï¼‰ï¼›è‹¥ä¸º None åˆ™è‡ªåŠ¨åˆ†é…
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        # è·å–æ¨¡å‹é…ç½®
        if primary_timeframe is None:
            primary_timeframe = self.config.PRIMARY_TIMEFRAME
        
        if version_id is None:
            version_id = allocate_version_id(models_dir=self.config.MODELS_DIR)
        # ç¡®ä¿ç‰ˆæœ¬ç›®å½•å­˜åœ¨ï¼šmodels/{version_id}/{symbol}/{timeframe}/
        version_dir = self.config.get_version_dir(version_id)
        symbol_tf_dir = os.path.join(version_dir, symbol, primary_timeframe)
        os.makedirs(symbol_tf_dir, exist_ok=True)
        
        model_config = self.config.get_model_config(primary_timeframe)
        timeframes = model_config["timeframes"]
        sequence_length = model_config["sequence_length"]
        lstm_units = model_config.get("lstm_units", self.config.LSTM_UNITS)
        dense_units = model_config.get("dense_units", self.config.DENSE_UNITS)
        dropout_rate = model_config.get("dropout_rate", self.config.DROPOUT_RATE)
        
        logger.info(f"="*80)
        logger.info(f"å¼€å§‹å®Œæ•´é‡è®­: {symbol} (primary_timeframe={primary_timeframe})")
        logger.info(f"  æ—¶é—´æ¡†æ¶: {timeframes}")
        logger.info(f"  åºåˆ—é•¿åº¦: {sequence_length}")
        logger.info(f"  LSTM å•å…ƒ: {lstm_units}, Dense å•å…ƒ: {dense_units}")
        logger.info(f"  Dropout: {dropout_rate}")
        logger.info(f"="*80)
        
        # 1. è·å–æ•°æ®
        logger.info("æ­¥éª¤ 1/6: è·å–å†å²æ•°æ®...")
        data = self.data_fetcher.fetch_full_training_data(
            symbol=symbol,
            timeframes=timeframes,
            days=self.config.FULL_RETRAIN_DAYS
        )
        # æ³¨æ„ï¼šæ•°æ®å·²è‡ªåŠ¨ä¿å­˜åˆ° SQLite ç¼“å­˜ä¸­ï¼Œæ— éœ€é¢å¤–ä¿å­˜
        
        # è¾“å‡º API ç»Ÿè®¡ä¿¡æ¯
        stats = self.data_fetcher.get_api_stats()
        logger.info(f"API è¯·æ±‚ç»Ÿè®¡: {stats}")
        
        # 2. ç‰¹å¾å·¥ç¨‹
        logger.info("æ­¥éª¤ 2/6: è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
        features = self.feature_engineer.combine_timeframe_features(
            data,
            primary_timeframe=primary_timeframe,
            symbol=symbol
        )
        
        # å¯é€‰ï¼šç‰¹å¾é€‰æ‹©
        features = self.feature_engineer.select_key_features(features)
        
        logger.info(f"ç‰¹å¾æ•°é‡: {len(features.columns)}, æ ·æœ¬æ•°: {len(features)}")
        
        # 3. æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®ï¼ˆå…³é”®æ­¥éª¤ï¼šé¿å…æ•°æ®æ³„æ¼ï¼‰
        logger.info("æ­¥éª¤ 3/6: æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®...")
        train_features, val_features, test_features = self._split_data_by_time(
            features,
            train_ratio=self.config.TRAIN_RATIO,
            val_ratio=self.config.VAL_RATIO,
            test_ratio=self.config.TEST_RATIO
        )
        
        # 4. HMM æ ‡æ³¨ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œé¿å…æ•°æ®æ³„æ¼ï¼‰
        logger.info("æ­¥éª¤ 4/6: HMM çŠ¶æ€æ ‡æ³¨ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼‰...")
        
        # åŠ è½½æ—§æ¨¡å‹çš„æ˜ å°„ï¼ˆç”¨äºæ¯”å¯¹ï¼‰- ä½¿ç”¨ PROD è·¯å¾„
        old_hmm_path = self.config.get_prod_hmm_path(symbol, primary_timeframe)
        old_mapping = None
        if os.path.exists(old_hmm_path):
            try:
                old_hmm = HMMRegimeLabeler.load(old_hmm_path)
                old_mapping = old_hmm.get_regime_mapping()
                logger.info(f"å·²åŠ è½½æ—§æ¨¡å‹æ˜ å°„ç”¨äºæ¯”å¯¹: {old_mapping}")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½æ—§æ¨¡å‹: {e}")
        
        # ä¿å­˜è·¯å¾„ä½¿ç”¨ç‰ˆæœ¬ç›®å½•
        hmm_path = self.config.get_hmm_path_for_version(version_id, symbol, primary_timeframe)
        
        hmm_labeler = HMMRegimeLabeler(
            n_states=self.config.N_STATES,
            n_components=self.config.N_PCA_COMPONENTS,
            primary_timeframe=primary_timeframe
        )
        
        # å¯é€‰ï¼šBIC éªŒè¯çŠ¶æ€æ•°é‡æ˜¯å¦åˆç†
        bic_validation = None
        if getattr(self.config, 'VALIDATE_N_STATES', False):
            logger.info("æ‰§è¡Œ BIC éªŒè¯ï¼ˆéªŒè¯çŠ¶æ€æ•°é‡æ˜¯å¦åˆç†ï¼‰...")
            bic_test_states = getattr(self.config, 'BIC_TEST_N_STATES', [4, 5, 6, 7, 8])
            bic_validation = hmm_labeler.validate_n_states(
                train_features, 
                n_states_range=bic_test_states
            )
            logger.info(f"BIC éªŒè¯ç»“æœ: {bic_validation['recommendation']}")
        
        # ä½¿ç”¨æ–°æ–¹æ³•ï¼šåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œåˆ†åˆ«é¢„æµ‹å„æ•°æ®é›†çš„æ ‡ç­¾
        train_states, val_states, test_states = hmm_labeler.fit_predict_split(
            train_features=train_features,
            val_features=val_features,
            test_features=test_features
        )
        
        # ========== å¤šæ­¥é¢„æµ‹æ ‡ç­¾ç”Ÿæˆ ==========
        # ä½¿ç”¨ forward-only filteringï¼ˆæ—  look-ahead biasï¼‰
        prediction_horizons = getattr(self.config, 'PREDICTION_HORIZONS', [1, 2, 3, 4])
        label_temperature = getattr(self.config, 'LABEL_TEMPERATURE', 1.5)
        
        logger.info(f"ç”Ÿæˆå¤šæ­¥é¢„æµ‹æ ‡ç­¾: horizons={prediction_horizons}, temperature={label_temperature}")
        
        # Forward filter ç”Ÿæˆæ»¤æ³¢åéªŒæ¦‚ç‡
        train_posteriors = hmm_labeler.forward_filter(train_features)
        val_posteriors = hmm_labeler.forward_filter(val_features)
        test_posteriors = hmm_labeler.forward_filter(test_features) if test_features is not None else None
        
        # ç”Ÿæˆå¤šæ­¥æ ‡ç­¾
        train_multistep_labels = hmm_labeler.generate_multistep_labels(
            train_posteriors, horizons=prediction_horizons, temperature=label_temperature
        )
        val_multistep_labels = hmm_labeler.generate_multistep_labels(
            val_posteriors, horizons=prediction_horizons, temperature=label_temperature
        )
        test_multistep_labels = None
        if test_posteriors is not None:
            test_multistep_labels = hmm_labeler.generate_multistep_labels(
                test_posteriors, horizons=prediction_horizons, temperature=label_temperature
            )
        
        # è‡ªåŠ¨æ˜ å°„ HMM çŠ¶æ€åˆ°è¯­ä¹‰åç§°ï¼ˆå…³é”®æ­¥éª¤ï¼ï¼‰
        # ä½¿ç”¨é…ç½®ä¸­çš„ç»å¯¹é˜ˆå€¼æŠ¤æ å‚æ•°
        regime_mapping = hmm_labeler.auto_map_regimes(
            train_features, 
            train_states,
            min_vol_for_spike=getattr(self.config, 'REGIME_MIN_VOL_FOR_SPIKE', 0.02),
            max_vol_for_squeeze=getattr(self.config, 'REGIME_MAX_VOL_FOR_SQUEEZE', 0.01),
            min_adx_for_strong_trend=getattr(self.config, 'REGIME_MIN_ADX_FOR_STRONG_TREND', 30),
            max_adx_for_squeeze=getattr(self.config, 'REGIME_MAX_ADX_FOR_SQUEEZE', 20)
        )
        logger.info(f"HMM çŠ¶æ€åˆ°è¯­ä¹‰åç§°çš„æ˜ å°„: {regime_mapping}")
        
        # æ£€æŸ¥çŠ¶æ€åˆ†å¸ƒæ˜¯å¦å¥åº·ï¼ˆéªŒè¯é›†/æµ‹è¯•é›†æ˜¯å¦ç¼ºå¤±æŸäº›çŠ¶æ€ï¼‰
        state_distribution_check = hmm_labeler.check_state_distribution(
            train_states=train_states,
            val_states=val_states,
            test_states=test_states,
            min_samples_per_state=getattr(self.config, 'MIN_SAMPLES_PER_STATE', 10),
            min_ratio_per_state=getattr(self.config, 'MIN_RATIO_PER_STATE', 0.01)
        )
        
        # ========== åŠ¨æ€è°ƒæ•´çŠ¶æ€æ•°é‡ ==========
        # å¦‚æœçŠ¶æ€åˆ†å¸ƒä¸å¥åº·ä¸”å¯ç”¨äº†è‡ªåŠ¨è°ƒæ•´ï¼Œå°è¯•ä¼˜åŒ–çŠ¶æ€æ•°é‡
        n_states_optimization = None
        auto_adjust_enabled = getattr(self.config, 'AUTO_ADJUST_N_STATES', False)
        
        if auto_adjust_enabled and not state_distribution_check['healthy']:
            missing_val = len(state_distribution_check['missing_states']['val'])
            low_ratio_val = len(state_distribution_check['low_sample_states']['val'])
            max_missing = getattr(self.config, 'MAX_MISSING_STATES_ALLOWED', 1)
            max_low_ratio = getattr(self.config, 'MAX_LOW_RATIO_STATES_ALLOWED', 2)
            
            if missing_val > max_missing or low_ratio_val > max_low_ratio:
                logger.info(f"ğŸ”„ çŠ¶æ€åˆ†å¸ƒä¸å¥åº·ï¼ˆç¼ºå¤±: {missing_val}, ä½å æ¯”: {low_ratio_val}ï¼‰ï¼Œå°è¯•è‡ªåŠ¨ä¼˜åŒ–çŠ¶æ€æ•°é‡...")
                
                n_states_optimization = hmm_labeler.auto_optimize_n_states(
                    train_features=train_features,
                    val_features=val_features,
                    test_features=test_features,
                    n_states_min=getattr(self.config, 'N_STATES_MIN', 4),
                    n_states_max=getattr(self.config, 'N_STATES_MAX', 8),
                    max_missing_allowed=max_missing,
                    max_low_ratio_allowed=max_low_ratio,
                    strategy=getattr(self.config, 'N_STATES_ADJUST_STRATEGY', 'decrease_first'),
                    min_samples_per_state=getattr(self.config, 'MIN_SAMPLES_PER_STATE', 10),
                    min_ratio_per_state=getattr(self.config, 'MIN_RATIO_PER_STATE', 0.01)
                )
                
                # å¦‚æœçŠ¶æ€æ•°é‡è¢«è°ƒæ•´ï¼Œéœ€è¦é‡æ–°è®­ç»ƒå’Œæ˜ å°„
                if n_states_optimization['adjusted']:
                    new_n_states = n_states_optimization['optimal_n_states']
                    logger.info(f"ä½¿ç”¨ä¼˜åŒ–åçš„çŠ¶æ€æ•°é‡ {new_n_states} é‡æ–°è®­ç»ƒ...")
                    
                    # é‡æ–°è®­ç»ƒ
                    train_states, val_states, test_states = hmm_labeler.retrain_with_n_states(
                        n_states=new_n_states,
                        train_features=train_features,
                        val_features=val_features,
                        test_features=test_features
                    )
                    
                    # é‡æ–°æ˜ å°„ï¼ˆä½¿ç”¨ä¼˜å…ˆçº§é€‰æ‹©åç§°ï¼‰
                    regime_mapping = hmm_labeler.auto_map_regimes(
                        train_features, 
                        train_states,
                        min_vol_for_spike=getattr(self.config, 'REGIME_MIN_VOL_FOR_SPIKE', 0.02),
                        max_vol_for_squeeze=getattr(self.config, 'REGIME_MAX_VOL_FOR_SQUEEZE', 0.01),
                        min_adx_for_strong_trend=getattr(self.config, 'REGIME_MIN_ADX_FOR_STRONG_TREND', 30),
                        max_adx_for_squeeze=getattr(self.config, 'REGIME_MAX_ADX_FOR_SQUEEZE', 20)
                    )
                    logger.info(f"ä¼˜åŒ–åçš„çŠ¶æ€æ˜ å°„: {regime_mapping}")
                    
                    # é‡æ–°æ£€æŸ¥åˆ†å¸ƒ
                    state_distribution_check = hmm_labeler.check_state_distribution(
                        train_states=train_states,
                        val_states=val_states,
                        test_states=test_states,
                        min_samples_per_state=getattr(self.config, 'MIN_SAMPLES_PER_STATE', 10),
                        min_ratio_per_state=getattr(self.config, 'MIN_RATIO_PER_STATE', 0.01)
                    )
                    
                    # âš ï¸ å…³é”®ä¿®å¤ï¼šçŠ¶æ€æ•°é‡è°ƒæ•´åï¼Œéœ€è¦é‡æ–°ç”Ÿæˆå¤šæ­¥é¢„æµ‹æ ‡ç­¾
                    # å› ä¸ºæ ‡ç­¾çš„ç»´åº¦å¿…é¡»ä¸æ–°çš„çŠ¶æ€æ•°é‡åŒ¹é…
                    logger.info(f"é‡æ–°ç”Ÿæˆå¤šæ­¥é¢„æµ‹æ ‡ç­¾ï¼ˆçŠ¶æ€æ•°é‡å·²ä» {n_states_optimization['original_n_states']} è°ƒæ•´ä¸º {new_n_states}ï¼‰...")
                    train_posteriors = hmm_labeler.forward_filter(train_features)
                    val_posteriors = hmm_labeler.forward_filter(val_features)
                    test_posteriors = hmm_labeler.forward_filter(test_features) if test_features is not None else None
                    
                    train_multistep_labels = hmm_labeler.generate_multistep_labels(
                        train_posteriors, horizons=prediction_horizons, temperature=label_temperature
                    )
                    val_multistep_labels = hmm_labeler.generate_multistep_labels(
                        val_posteriors, horizons=prediction_horizons, temperature=label_temperature
                    )
                    test_multistep_labels = None
                    if test_posteriors is not None:
                        test_multistep_labels = hmm_labeler.generate_multistep_labels(
                            test_posteriors, horizons=prediction_horizons, temperature=label_temperature
                        )
                    
                    # è¾“å‡ºçŠ¶æ€è°ƒæ•´æ€»ç»“
                    self._log_state_adjustment_summary(
                        original_n_states=n_states_optimization['original_n_states'],
                        new_n_states=new_n_states,
                        new_mapping=regime_mapping
                    )
        
        # æ–°æ—§æ˜ å°„æ¯”å¯¹ï¼ˆæ£€æµ‹è¯­ä¹‰æ¼‚ç§»ï¼‰
        mapping_comparison = None
        if old_mapping is not None:
            mapping_diff_threshold = getattr(self.config, 'MAPPING_DIFF_THRESHOLD', 2)
            mapping_comparison = hmm_labeler.compare_mapping(old_mapping, threshold=mapping_diff_threshold)
            logger.info(f"æ˜ å°„æ¯”å¯¹ç»“æœ: {mapping_comparison['message']}")
        
        # åˆ†æ regime ç¨³å®šæ€§ï¼ˆæ£€æµ‹å¼‚å¸¸é¢‘ç¹åˆ‡æ¢ï¼‰
        switch_threshold = getattr(self.config, 'REGIME_SWITCH_WARNING_THRESHOLD', 10)
        stability_analysis = hmm_labeler.analyze_regime_stability(train_states, switch_threshold)
        
        # è®¡ç®—é©»ç•™æ—¶é—´åˆ†å¸ƒ
        dwell_times = hmm_labeler.compute_dwell_times(train_states)
        logger.info(f"çŠ¶æ€é©»ç•™æ—¶é—´åˆ†å¸ƒ: {dwell_times}")
        
        # ä¿å­˜ HMM æ¨¡å‹ï¼ˆåŒ…å«çŠ¶æ€æ˜ å°„ã€profilesã€è½¬ç§»çŸ©é˜µç­‰ï¼‰
        hmm_labeler.save(hmm_path)
        
        # åˆ†æå¸‚åœºçŠ¶æ€ï¼ˆåªç”¨è®­ç»ƒé›†åˆ†æï¼Œé¿å…æ³„æ¼ï¼‰
        regime_analysis = hmm_labeler.analyze_regimes(train_features, train_states)
        # æ·»åŠ è¯­ä¹‰åç§°åˆ°åˆ†æç»“æœ
        regime_analysis['regime_name'] = regime_analysis['state'].map(regime_mapping)
        logger.info(f"\nè®­ç»ƒé›†å¸‚åœºçŠ¶æ€åˆ†æ:\n{regime_analysis}")
        
        # 5. å‡†å¤‡ LSTM è®­ç»ƒæ•°æ®
        logger.info("æ­¥éª¤ 5/6: å‡†å¤‡ LSTM å¤šæ­¥é¢„æµ‹è®­ç»ƒæ•°æ®...")
        
        # è·å–æŸå¤±æƒé‡é…ç½®
        horizon_loss_weights = getattr(self.config, 'HORIZON_LOSS_WEIGHTS', {
            't+1': 1.0, 't+2': 0.8, 't+3': 0.6, 't+4': 0.4
        })
        
        lstm_classifier = LSTMRegimeClassifier(
            n_states=hmm_labeler.n_states,  # ä½¿ç”¨ HMM çš„å®é™…çŠ¶æ€æ•°é‡ï¼ˆå¯èƒ½è¢«åŠ¨æ€è°ƒæ•´ï¼‰
            sequence_length=sequence_length,  # ä½¿ç”¨æ¨¡å‹é…ç½®çš„åºåˆ—é•¿åº¦
            lstm_units=lstm_units,  # ä½¿ç”¨æ¨¡å‹é…ç½®çš„ LSTM å•å…ƒæ•°
            dense_units=dense_units,  # ä½¿ç”¨æ¨¡å‹é…ç½®çš„ Dense å•å…ƒæ•°
            dropout_rate=dropout_rate,
            l2_lambda=self.config.L2_LAMBDA,
            use_batch_norm=self.config.USE_BATCH_NORM,
            learning_rate=self.config.LEARNING_RATE,
            prediction_horizons=prediction_horizons,
            horizon_loss_weights=horizon_loss_weights
        )
        
        # ä½¿ç”¨å¤šæ­¥æ•°æ®å‡†å¤‡æ–¹æ³•
        X_train, y_train_dict, X_val, y_val_dict, X_test, y_test_dict = lstm_classifier.prepare_multistep_data_split(
            train_features=train_features,
            train_labels=train_multistep_labels,
            val_features=val_features,
            val_labels=val_multistep_labels,
            test_features=test_features,
            test_labels=test_multistep_labels
        )
        
        # 6. è®­ç»ƒ LSTM
        logger.info("æ­¥éª¤ 6/6: è®­ç»ƒ LSTM å¤šæ­¥é¢„æµ‹æ¨¡å‹...")
        model_path = self.config.get_model_path_for_version(version_id, symbol, "lstm", primary_timeframe)
        
        # ä½¿ç”¨å¤šæ­¥è®­ç»ƒæ–¹æ³•
        history = lstm_classifier.train_multistep(
            X_train, y_train_dict,
            X_val, y_val_dict,  # éªŒè¯é›†ç”¨äºæ—©åœ
            epochs=self.config.EPOCHS,
            batch_size=self.config.BATCH_SIZE,
            early_stopping_patience=self.config.EARLY_STOPPING_PATIENCE,
            lr_reduce_patience=self.config.LR_REDUCE_PATIENCE,
            model_path=model_path,
            use_class_weight=self.config.USE_CLASS_WEIGHT
        )
        
        # åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆè¿™æ‰æ˜¯çœŸå®çš„æ³›åŒ–æ€§èƒ½ï¼‰
        logger.info("åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°å¤šæ­¥é¢„æµ‹æ¨¡å‹...")
        eval_results = {}
        val_eval = {}
        
        if X_test is not None and y_test_dict is not None:
            # è¯„ä¼° t+1 é¢„æµ‹ï¼ˆä¸»è¦æŒ‡æ ‡ï¼‰
            y_test_t1 = y_test_dict['t+1']
            y_pred_t1 = lstm_classifier.predict(X_test)
            from sklearn.metrics import accuracy_score
            test_acc_t1 = accuracy_score(y_test_t1, y_pred_t1)
            eval_results['accuracy'] = test_acc_t1
            eval_results['loss'] = 0.0  # éœ€è¦ä»æ¨¡å‹è·å–
            logger.info(f"ğŸ¯ æµ‹è¯•é›† t+1 å‡†ç¡®ç‡: {test_acc_t1:.4f} (è¿™æ˜¯çœŸå®çš„æ³›åŒ–æ€§èƒ½)")
            
            # è¯„ä¼°å…¶ä»– horizonï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            multistep_predictions = lstm_classifier.predict_multistep(X_test)
            for h in prediction_horizons:
                if h > 1:
                    # å¯¹äºè½¯æ ‡ç­¾ï¼Œæ¯”è¾ƒ argmax
                    y_true_h = np.argmax(y_test_dict[f't+{h}'], axis=1)
                    y_pred_h = np.argmax(multistep_predictions[f't+{h}'], axis=1)
                    acc_h = accuracy_score(y_true_h, y_pred_h)
                    logger.info(f"    æµ‹è¯•é›† t+{h} å‡†ç¡®ç‡: {acc_h:.4f}")
        else:
            # ä½¿ç”¨éªŒè¯é›†è¯„ä¼°
            y_val_t1 = y_val_dict['t+1']
            y_pred_val_t1 = lstm_classifier.predict(X_val)
            from sklearn.metrics import accuracy_score
            val_acc_t1 = accuracy_score(y_val_t1, y_pred_val_t1)
            eval_results['accuracy'] = val_acc_t1
            eval_results['loss'] = 0.0
            logger.warning("âš ï¸ æ²¡æœ‰ç‹¬ç«‹æµ‹è¯•é›†ï¼Œä½¿ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆç»“æœå¯èƒ½åä¹è§‚ï¼‰")
        
        # éªŒè¯é›†å‡†ç¡®ç‡ä½œä¸ºå‚è€ƒ
        y_val_t1 = y_val_dict['t+1']
        y_pred_val = lstm_classifier.predict(X_val)
        from sklearn.metrics import accuracy_score
        val_acc = accuracy_score(y_val_t1, y_pred_val)
        val_eval['accuracy'] = val_acc
        logger.info(f"éªŒè¯é›† t+1 å‡†ç¡®ç‡: {val_acc:.4f}")
        
        # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        scaler_path = self.config.get_scaler_path_for_version(version_id, symbol, primary_timeframe)
        lstm_classifier.save(model_path, scaler_path)
        
        register_version(version_id, db_path=os.path.join(self.config.DATA_DIR, "model_registry.db"))
        try:
            cron_mgr = ForwardTestCronManager._instance
            forward_test_on_training_finished(symbol, primary_timeframe, version_id, self.config, cron_manager=cron_mgr)
        except Exception as e:
            logger.warning(f"Forward test enrollment failed (training result unchanged): {e}")
        logger.info(f"å®Œæ•´é‡è®­å®Œæˆ: {symbol} (primary_timeframe={primary_timeframe}) version_id={version_id}")
        logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {eval_results['accuracy']:.4f}")
        
        return {
            'symbol': symbol,
            'primary_timeframe': primary_timeframe,  # ä¸»æ—¶é—´æ¡†æ¶
            'version_id': version_id,
            'training_type': 'full_retrain',
            'timestamp': datetime.now(),
            'test_accuracy': eval_results['accuracy'],
            'val_accuracy': val_eval['accuracy'],
            'test_loss': eval_results.get('loss', 0.0),
            'regime_analysis': regime_analysis,
            'regime_mapping': regime_mapping,  # HMM çŠ¶æ€åˆ°è¯­ä¹‰åç§°çš„æ˜ å°„
            'mapping_comparison': mapping_comparison,  # æ–°æ—§æ˜ å°„æ¯”å¯¹ç»“æœ
            'stability_analysis': stability_analysis,  # regime ç¨³å®šæ€§åˆ†æ
            'state_distribution_check': state_distribution_check,  # çŠ¶æ€åˆ†å¸ƒå¥åº·æ£€æŸ¥
            'dwell_times': dwell_times,  # çŠ¶æ€é©»ç•™æ—¶é—´åˆ†å¸ƒ
            'training_bic': hmm_labeler.training_bic_,  # HMM è®­ç»ƒçš„ BIC å€¼
            'bic_validation': bic_validation,  # BIC çŠ¶æ€æ•°é‡éªŒè¯ç»“æœ
            'n_states_optimization': n_states_optimization,  # åŠ¨æ€çŠ¶æ€æ•°é‡ä¼˜åŒ–ç»“æœ
            'final_n_states': hmm_labeler.n_states,  # æœ€ç»ˆä½¿ç”¨çš„çŠ¶æ€æ•°é‡
            'sequence_length': sequence_length,  # åºåˆ—é•¿åº¦
            'prediction_horizons': prediction_horizons,  # å¤šæ­¥é¢„æµ‹æ­¥æ•°
            'is_multistep': lstm_classifier.is_multistep,  # æ˜¯å¦å¤šæ­¥æ¨¡å‹
            'history': history,
            'data_split': {
                'train_samples': len(train_features),
                'val_samples': len(val_features),
                'test_samples': len(test_features)
            }
        }
    
    def incremental_train(self, symbol: str, primary_timeframe: str = None, version_id: str = None) -> Dict:
        """
        å¢é‡è®­ç»ƒï¼ˆåœ¨ç°æœ‰æ¨¡å‹åŸºç¡€ä¸Šï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹
            primary_timeframe: ä¸»æ—¶é—´æ¡†æ¶ï¼ˆå¦‚ "5m", "15m" æˆ– "1h"ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            version_id: ç‰ˆæœ¬ç›®å½• idï¼›è‹¥ä¸º None åˆ™è‡ªåŠ¨åˆ†é…
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        # è·å–æ¨¡å‹é…ç½®
        if primary_timeframe is None:
            primary_timeframe = self.config.PRIMARY_TIMEFRAME
        
        if version_id is None:
            version_id = allocate_version_id(models_dir=self.config.MODELS_DIR)
        # ç¡®ä¿ç‰ˆæœ¬ç›®å½•å­˜åœ¨
        symbol_tf_dir = os.path.join(self.config.get_version_dir(version_id), symbol, primary_timeframe)
        os.makedirs(symbol_tf_dir, exist_ok=True)
        
        model_config = self.config.get_model_config(primary_timeframe)
        timeframes = model_config["timeframes"]
        
        logger.info(f"="*80)
        logger.info(f"å¼€å§‹å¢é‡è®­ç»ƒ: {symbol} (primary_timeframe={primary_timeframe}) version_id={version_id}")
        logger.info(f"="*80)
        
        # 1. è·å–æœ€æ–°æ•°æ®
        logger.info("æ­¥éª¤ 1/4: è·å–æœ€æ–°æ•°æ®...")
        data = self.data_fetcher.fetch_latest_data(
            symbol=symbol,
            timeframes=timeframes,
            days=self.config.INCREMENTAL_TRAIN_DAYS
        )
        
        # è¾“å‡º API ç»Ÿè®¡ä¿¡æ¯
        stats = self.data_fetcher.get_api_stats()
        logger.info(f"API è¯·æ±‚ç»Ÿè®¡: {stats}")
        
        # 2. ç‰¹å¾å·¥ç¨‹
        logger.info("æ­¥éª¤ 2/4: è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
        features = self.feature_engineer.combine_timeframe_features(
            data,
            primary_timeframe=primary_timeframe,
            symbol=symbol
        )
        
        # 3. åŠ è½½ HMM æ¨¡å‹å¹¶æ ‡æ³¨ï¼ˆä» PROD è·¯å¾„åŠ è½½ï¼‰
        logger.info("æ­¥éª¤ 3/4: HMM çŠ¶æ€æ ‡æ³¨...")
        hmm_path_load = self.config.get_prod_hmm_path(symbol, primary_timeframe)
        
        if not os.path.exists(hmm_path_load):
            logger.warning(f"HMM æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†æ‰§è¡Œå®Œæ•´é‡è®­: {hmm_path_load}")
            return self.full_retrain(symbol, primary_timeframe, version_id=version_id)
        
        hmm_labeler = HMMRegimeLabeler.load(hmm_path_load)
        
        # åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆä¸å®Œæ•´è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        features_before_selection = features.copy()
        features = self.feature_engineer.select_key_features(features)
        
        # å¦‚æœæ¨¡å‹ä¿å­˜äº†ç‰¹å¾åç§°ï¼Œç¡®ä¿ç‰¹å¾ä¸€è‡´
        if hmm_labeler.feature_names_ is not None:
            # æ£€æŸ¥ç‰¹å¾æ˜¯å¦åŒ¹é…
            missing_features = set(hmm_labeler.feature_names_) - set(features.columns)
            extra_features = set(features.columns) - set(hmm_labeler.feature_names_)
            
            if missing_features or extra_features:
                logger.warning(
                    f"ç‰¹å¾é€‰æ‹©ç»“æœä¸ä¸€è‡´ï¼\n"
                    f"  è®­ç»ƒæ—¶ç‰¹å¾æ•°: {len(hmm_labeler.feature_names_)}\n"
                    f"  å½“å‰ç‰¹å¾æ•°: {len(features.columns)}\n"
                    f"  ç¼ºå°‘ç‰¹å¾: {len(missing_features)} ä¸ª\n"
                    f"  å¤šä½™ç‰¹å¾: {len(extra_features)} ä¸ª"
                )
                # predict æ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†ç‰¹å¾å¯¹é½
        else:
            # æ—§ç‰ˆæœ¬æ¨¡å‹ï¼šæ£€æŸ¥ç‰¹å¾æ•°é‡
            expected_features = (
                hmm_labeler.scaler.n_features_in_ 
                if hasattr(hmm_labeler.scaler, 'n_features_in_') 
                else None
            )
            if expected_features and len(features.columns) != expected_features:
                logger.error(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼è®­ç»ƒæ—¶: {expected_features} ä¸ªç‰¹å¾, "
                    f"å½“å‰: {len(features.columns)} ä¸ªç‰¹å¾\n"
                    f"è¿™æ˜¯æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ä¿å­˜ç‰¹å¾åç§°ã€‚"
                )
                raise ValueError(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ã€‚è¯·é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§ã€‚"
                )
        
        states = hmm_labeler.predict(features)
        
        # 4. åŠ è½½ LSTM æ¨¡å‹å¹¶å¢é‡è®­ç»ƒï¼ˆä» PROD è·¯å¾„åŠ è½½ï¼‰
        logger.info("æ­¥éª¤ 4/4: LSTM å¢é‡è®­ç»ƒ...")
        model_path_load = self.config.get_prod_model_path(symbol, "lstm", primary_timeframe)
        scaler_path_load = self.config.get_prod_scaler_path(symbol, primary_timeframe)
        
        if not os.path.exists(model_path_load):
            logger.warning(f"LSTM æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†æ‰§è¡Œå®Œæ•´é‡è®­: {model_path_load}")
            return self.full_retrain(symbol, primary_timeframe, version_id=version_id)
        
        lstm_classifier = LSTMRegimeClassifier.load(model_path_load, scaler_path_load)
        
        # å¯¹é½ç‰¹å¾ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        # ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ç‰¹å¾åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ scaler çš„ feature_names_in_
        feature_names = lstm_classifier.feature_names_
        if feature_names is None and hasattr(lstm_classifier.scaler, 'feature_names_in_'):
            feature_names = list(lstm_classifier.scaler.feature_names_in_)
            logger.info(f"ä½¿ç”¨ scaler çš„ç‰¹å¾åç§°: {len(feature_names)} ä¸ªç‰¹å¾")
        
        if feature_names is not None:
            # ç¡®ä¿ç‰¹å¾é¡ºåºå’Œæ•°é‡ä¸è®­ç»ƒæ—¶ä¸€è‡´
            missing_features = set(feature_names) - set(features.columns)
            extra_features = set(features.columns) - set(feature_names)
            
            if missing_features or extra_features:
                logger.warning(
                    f"ç‰¹å¾ä¸ä¸€è‡´ï¼\n"
                    f"  è®­ç»ƒæ—¶ç‰¹å¾æ•°: {len(feature_names)}\n"
                    f"  å½“å‰ç‰¹å¾æ•°: {len(features.columns)}\n"
                    f"  ç¼ºå°‘ç‰¹å¾: {len(missing_features)} ä¸ª\n"
                    f"  å¤šä½™ç‰¹å¾: {len(extra_features)} ä¸ª"
                )
                if missing_features:
                    logger.warning(f"  ç¼ºå°‘çš„ç‰¹å¾: {missing_features}")
                if extra_features:
                    logger.warning(f"  å¤šä½™çš„ç‰¹å¾: {extra_features}")
                
                # å¯¹é½ç‰¹å¾ï¼šæ·»åŠ ç¼ºå¤±çš„ç‰¹å¾ï¼ˆå¡«å……0ï¼‰ï¼Œç§»é™¤å¤šä½™çš„ç‰¹å¾
                features_aligned = features.reindex(columns=feature_names, fill_value=0)
                logger.info(f"ç‰¹å¾å·²å¯¹é½: {len(features_aligned.columns)} ä¸ªç‰¹å¾")
            else:
                # ç‰¹å¾åç§°ä¸€è‡´ï¼Œä½†éœ€è¦ç¡®ä¿é¡ºåºä¸€è‡´
                features_aligned = features[feature_names]
        else:
            # æ—§ç‰ˆæœ¬æ¨¡å‹ï¼šåªæ£€æŸ¥ç‰¹å¾æ•°é‡
            expected_features = (
                lstm_classifier.scaler.n_features_in_ 
                if hasattr(lstm_classifier.scaler, 'n_features_in_') 
                else None
            )
            if expected_features and len(features.columns) != expected_features:
                logger.error(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼è®­ç»ƒæ—¶: {expected_features} ä¸ªç‰¹å¾, "
                    f"å½“å‰: {len(features.columns)} ä¸ªç‰¹å¾\n"
                    f"è¿™æ˜¯æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ä¿å­˜ç‰¹å¾åç§°ã€‚"
                )
                raise ValueError(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ã€‚è¯·é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§ã€‚"
                )
            features_aligned = features
        
        # å‡†å¤‡å¢é‡è®­ç»ƒæ•°æ®
        features_scaled = lstm_classifier.scaler.transform(features_aligned)
        
        X, y = [], []
        for i in range(len(features_scaled) - lstm_classifier.sequence_length):
            X.append(features_scaled[i:i+lstm_classifier.sequence_length])
            y.append(states[i+lstm_classifier.sequence_length])
        
        X = np.array(X)
        y = np.array(y)
        
        # æ‰§è¡Œå¢é‡è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†å’Œæ—©åœï¼‰
        lstm_classifier.incremental_train(
            X, y,
            epochs=self.config.INCREMENTAL_EPOCHS,
            batch_size=self.config.BATCH_SIZE,
            learning_rate=self.config.INCREMENTAL_LEARNING_RATE,
            validation_split=self.config.INCREMENTAL_VALIDATION_SPLIT,
            early_stopping_patience=self.config.INCREMENTAL_EARLY_STOPPING_PATIENCE,
            use_class_weight=self.config.USE_CLASS_WEIGHT
        )
        
        # ä¿å­˜æ›´æ–°åçš„æ¨¡å‹åˆ°ç‰ˆæœ¬ç›®å½•
        model_path = self.config.get_model_path_for_version(version_id, symbol, "lstm", primary_timeframe)
        scaler_path = self.config.get_scaler_path_for_version(version_id, symbol, primary_timeframe)
        lstm_classifier.save(model_path, scaler_path)
        
        register_version(version_id, db_path=os.path.join(self.config.DATA_DIR, "model_registry.db"))
        try:
            cron_mgr = ForwardTestCronManager._instance
            forward_test_on_training_finished(symbol, primary_timeframe, version_id, self.config, cron_manager=cron_mgr)
        except Exception as e:
            logger.warning(f"Forward test enrollment failed (training result unchanged): {e}")
        logger.info(f"å¢é‡è®­ç»ƒå®Œæˆ: {symbol} (primary_timeframe={primary_timeframe}) version_id={version_id}")
        
        return {
            'symbol': symbol,
            'primary_timeframe': primary_timeframe,
            'version_id': version_id,
            'training_type': 'incremental',
            'timestamp': datetime.now(),
            'samples_used': len(X)
        }
    
    def train_all_symbols(self, training_type: str = 'full', primary_timeframe: str = None) -> Dict:
        """
        è®­ç»ƒæ‰€æœ‰äº¤æ˜“å¯¹ï¼ˆä¸€ä¸ª version_id å¯¹åº”æœ¬æ¬¡è°ƒç”¨çš„æ‰€æœ‰ symbolï¼‰
        
        Args:
            training_type: 'full' æˆ– 'incremental'
            primary_timeframe: ä¸»æ—¶é—´æ¡†æ¶ï¼ˆå¦‚ "5m", "15m" æˆ– "1h"ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            
        Returns:
            æ‰€æœ‰äº¤æ˜“å¯¹çš„è®­ç»ƒç»“æœ
        """
        version_id = allocate_version_id(models_dir=self.config.MODELS_DIR)
        results = {}
        
        for symbol in self.config.SYMBOLS:
            try:
                if training_type == 'full':
                    result = self.full_retrain(symbol, primary_timeframe, version_id=version_id)
                else:
                    result = self.incremental_train(symbol, primary_timeframe, version_id=version_id)
                
                results[symbol] = result
                
            except Exception as e:
                logger.error(f"è®­ç»ƒ {symbol} æ—¶å‡ºé”™: {e}", exc_info=True)
                results[symbol] = {'error': str(e)}
        
        return results
    
    def train_multi_timeframe_models(
        self, 
        symbol: str, 
        timeframes: list = None, 
        training_type: str = 'full',
        version_id: str = None
    ) -> Dict:
        """
        ä¸ºå•ä¸ªäº¤æ˜“å¯¹è®­ç»ƒå¤šä¸ªæ—¶é—´æ¡†æ¶çš„æ¨¡å‹ï¼ˆä¸€ä¸ª version_id å¯¹åº”æœ¬æ¬¡è°ƒç”¨çš„æ‰€æœ‰ timeframeï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹
            timeframes: è¦è®­ç»ƒçš„æ—¶é—´æ¡†æ¶åˆ—è¡¨ï¼ˆå¦‚ ["5m", "15m", "1h"]ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ ENABLED_MODELS
            training_type: 'full' æˆ– 'incremental'
            version_id: ç‰ˆæœ¬ç›®å½• idï¼›è‹¥ä¸º None åˆ™è‡ªåŠ¨åˆ†é…
            
        Returns:
            å„æ—¶é—´æ¡†æ¶çš„è®­ç»ƒç»“æœ
        """
        if timeframes is None:
            timeframes = self.config.ENABLED_MODELS
        
        if version_id is None:
            version_id = allocate_version_id(models_dir=self.config.MODELS_DIR)
        
        results = {}
        
        for tf in timeframes:
            logger.info(f"\n{'='*80}")
            logger.info(f"è®­ç»ƒ {symbol} çš„ {tf} æ¨¡å‹... (version_id={version_id})")
            logger.info(f"{'='*80}\n")
            
            try:
                if training_type == 'full':
                    result = self.full_retrain(symbol, primary_timeframe=tf, version_id=version_id)
                else:
                    result = self.incremental_train(symbol, primary_timeframe=tf, version_id=version_id)
                
                results[tf] = result
                
            except Exception as e:
                logger.error(f"è®­ç»ƒ {symbol} çš„ {tf} æ¨¡å‹æ—¶å‡ºé”™: {e}", exc_info=True)
                results[tf] = {'error': str(e)}
        
        return results
    
    def train_all_multi_timeframe(
        self, 
        timeframes: list = None, 
        training_type: str = 'full'
    ) -> Dict:
        """
        ä¸ºæ‰€æœ‰äº¤æ˜“å¯¹è®­ç»ƒå¤šä¸ªæ—¶é—´æ¡†æ¶çš„æ¨¡å‹ï¼ˆä¸€ä¸ª version_id å¯¹åº”æœ¬æ¬¡è°ƒç”¨çš„å…¨éƒ¨ symbolÃ—timeframeï¼‰
        
        Args:
            timeframes: è¦è®­ç»ƒçš„æ—¶é—´æ¡†æ¶åˆ—è¡¨ï¼ˆå¦‚ ["5m", "15m", "1h"]ï¼‰ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ ENABLED_MODELS
            training_type: 'full' æˆ– 'incremental'
            
        Returns:
            {symbol: {timeframe: result}} æ ¼å¼çš„è®­ç»ƒç»“æœ
        """
        if timeframes is None:
            timeframes = self.config.ENABLED_MODELS
        
        version_id = allocate_version_id(models_dir=self.config.MODELS_DIR)
        results = {}
        
        for symbol in self.config.SYMBOLS:
            logger.info(f"\n{'#'*80}")
            logger.info(f"å¼€å§‹è®­ç»ƒ {symbol} çš„æ‰€æœ‰æ—¶é—´æ¡†æ¶æ¨¡å‹: {timeframes} (version_id={version_id})")
            logger.info(f"{'#'*80}\n")
            
            results[symbol] = self.train_multi_timeframe_models(
                symbol, timeframes, training_type, version_id=version_id
            )
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    TrainingConfig.ensure_dirs()
    
    # åˆ›å»ºè®­ç»ƒç®¡é“
    pipeline = TrainingPipeline(TrainingConfig)
    
    # ç¤ºä¾‹ï¼šå®Œæ•´é‡è®­ BTC
    result = pipeline.full_retrain("BTCUSDT")
    
    logger.info(f"\nè®­ç»ƒç»“æœ: {result}")


if __name__ == "__main__":
    main()
