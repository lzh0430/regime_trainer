"""
ä¸»è®­ç»ƒç®¡é“ - åè°ƒæ•°æ®è·å–ã€ç‰¹å¾å·¥ç¨‹ã€HMM å’Œ LSTM è®­ç»ƒ

ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜ï¼š
1. å…ˆæŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®ä¸º train/val/test
2. HMM åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼ˆscaler, PCA, HMM å‚æ•°ï¼‰
3. ç”¨è®­ç»ƒå¥½çš„ HMM å¯¹éªŒè¯é›†å’Œæµ‹è¯•é›†è¿›è¡Œé¢„æµ‹ï¼ˆæ— æ•°æ®æ³„æ¼ï¼‰
4. LSTM ä½¿ç”¨ç‹¬ç«‹çš„éªŒè¯é›†å’Œæµ‹è¯•é›†
"""
import logging
import os
from datetime import datetime
from typing import Dict, Tuple
import pandas as pd
import numpy as np

from config import TrainingConfig
from data_fetcher import BinanceDataFetcher
from feature_engineering import FeatureEngineer
from hmm_trainer import HMMRegimeLabeler
from lstm_trainer import LSTMRegimeClassifier

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class TrainingPipeline:
    """è®­ç»ƒç®¡é“"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.data_fetcher = BinanceDataFetcher(
            api_key=config.BINANCE_API_KEY,
            api_secret=config.BINANCE_API_SECRET
        )
        self.feature_engineer = FeatureEngineer(cache_manager=self.data_fetcher.cache_manager)
    
    def _split_data_by_time(
        self, 
        features: pd.DataFrame, 
        train_ratio: float = 0.7,
        val_ratio: float = 0.15,
        test_ratio: float = 0.15
    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®ä¸º train/val/test
        
        Args:
            features: å®Œæ•´ç‰¹å¾ DataFrame
            train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
            val_ratio: éªŒè¯é›†æ¯”ä¾‹
            test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
            
        Returns:
            (train_features, val_features, test_features)
        """
        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
            "train_ratio + val_ratio + test_ratio å¿…é¡»ç­‰äº 1.0"
        
        n = len(features)
        train_end = int(n * train_ratio)
        val_end = int(n * (train_ratio + val_ratio))
        
        train_features = features.iloc[:train_end]
        val_features = features.iloc[train_end:val_end]
        test_features = features.iloc[val_end:]
        
        logger.info(f"æ—¶é—´åºåˆ—æ•°æ®åˆ’åˆ†:")
        logger.info(f"  è®­ç»ƒé›†: {len(train_features)} è¡Œ ({train_ratio:.0%})")
        logger.info(f"  éªŒè¯é›†: {len(val_features)} è¡Œ ({val_ratio:.0%})")
        logger.info(f"  æµ‹è¯•é›†: {len(test_features)} è¡Œ ({test_ratio:.0%})")
        
        if len(train_features) > 0 and len(test_features) > 0:
            logger.info(f"  è®­ç»ƒé›†æ—¶é—´èŒƒå›´: {train_features.index.min()} ~ {train_features.index.max()}")
            logger.info(f"  éªŒè¯é›†æ—¶é—´èŒƒå›´: {val_features.index.min()} ~ {val_features.index.max()}")
            logger.info(f"  æµ‹è¯•é›†æ—¶é—´èŒƒå›´: {test_features.index.min()} ~ {test_features.index.max()}")
        
        return train_features, val_features, test_features
    
    def full_retrain(self, symbol: str) -> Dict:
        """
        å®Œæ•´é‡è®­ï¼ˆä»é›¶å¼€å§‹ï¼‰
        
        ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜ï¼š
        1. å…ˆæŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®ä¸º train/val/test
        2. HMM åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
        3. LSTM ä½¿ç”¨ç‹¬ç«‹çš„éªŒè¯é›†å’Œæµ‹è¯•é›†
        
        Args:
            symbol: äº¤æ˜“å¯¹
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        logger.info(f"="*80)
        logger.info(f"å¼€å§‹å®Œæ•´é‡è®­: {symbol}")
        logger.info(f"="*80)
        
        # 1. è·å–æ•°æ®
        logger.info("æ­¥éª¤ 1/6: è·å–å†å²æ•°æ®...")
        data = self.data_fetcher.fetch_full_training_data(
            symbol=symbol,
            timeframes=self.config.TIMEFRAMES,
            days=self.config.FULL_RETRAIN_DAYS
        )
        # æ³¨æ„ï¼šæ•°æ®å·²è‡ªåŠ¨ä¿å­˜åˆ° SQLite ç¼“å­˜ä¸­ï¼Œæ— éœ€é¢å¤–ä¿å­˜
        
        # è¾“å‡º API ç»Ÿè®¡ä¿¡æ¯
        stats = self.data_fetcher.get_api_stats()
        logger.info(f"API è¯·æ±‚ç»Ÿè®¡: {stats}")
        
        # 2. ç‰¹å¾å·¥ç¨‹
        logger.info("æ­¥éª¤ 2/6: è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
        features = self.feature_engineer.combine_timeframe_features(
            data,
            primary_timeframe=self.config.PRIMARY_TIMEFRAME,
            symbol=symbol
        )
        
        # å¯é€‰ï¼šç‰¹å¾é€‰æ‹©
        features = self.feature_engineer.select_key_features(features)
        
        logger.info(f"ç‰¹å¾æ•°é‡: {len(features.columns)}, æ ·æœ¬æ•°: {len(features)}")
        
        # 3. æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®ï¼ˆå…³é”®æ­¥éª¤ï¼šé¿å…æ•°æ®æ³„æ¼ï¼‰
        logger.info("æ­¥éª¤ 3/6: æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®...")
        train_features, val_features, test_features = self._split_data_by_time(
            features,
            train_ratio=self.config.TRAIN_RATIO,
            val_ratio=self.config.VAL_RATIO,
            test_ratio=self.config.TEST_RATIO
        )
        
        # 4. HMM æ ‡æ³¨ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œé¿å…æ•°æ®æ³„æ¼ï¼‰
        logger.info("æ­¥éª¤ 4/6: HMM çŠ¶æ€æ ‡æ³¨ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼‰...")
        hmm_labeler = HMMRegimeLabeler(
            n_states=self.config.N_STATES,
            n_components=self.config.N_PCA_COMPONENTS
        )
        
        # ä½¿ç”¨æ–°æ–¹æ³•ï¼šåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œåˆ†åˆ«é¢„æµ‹å„æ•°æ®é›†çš„æ ‡ç­¾
        train_states, val_states, test_states = hmm_labeler.fit_predict_split(
            train_features=train_features,
            val_features=val_features,
            test_features=test_features
        )
        
        # ä¿å­˜ HMM æ¨¡å‹
        hmm_path = self.config.get_hmm_path(symbol)
        hmm_labeler.save(hmm_path)
        
        # åˆ†æå¸‚åœºçŠ¶æ€ï¼ˆåªç”¨è®­ç»ƒé›†åˆ†æï¼Œé¿å…æ³„æ¼ï¼‰
        regime_analysis = hmm_labeler.analyze_regimes(train_features, train_states)
        logger.info(f"\nè®­ç»ƒé›†å¸‚åœºçŠ¶æ€åˆ†æ:\n{regime_analysis}")
        
        # 5. å‡†å¤‡ LSTM è®­ç»ƒæ•°æ®
        logger.info("æ­¥éª¤ 5/6: å‡†å¤‡ LSTM è®­ç»ƒæ•°æ®...")
        lstm_classifier = LSTMRegimeClassifier(
            n_states=self.config.N_STATES,
            sequence_length=self.config.SEQUENCE_LENGTH,
            lstm_units=self.config.LSTM_UNITS,
            dense_units=self.config.DENSE_UNITS,
            dropout_rate=self.config.DROPOUT_RATE,
            l2_lambda=self.config.L2_LAMBDA,
            use_batch_norm=self.config.USE_BATCH_NORM,
            learning_rate=self.config.LEARNING_RATE
        )
        
        # ä½¿ç”¨æ–°æ–¹æ³•ï¼šæ”¯æŒ train/val/test ä¸‰åˆ†
        X_train, y_train, X_val, y_val, X_test, y_test = lstm_classifier.prepare_data_split(
            train_features=train_features,
            train_labels=train_states,
            val_features=val_features,
            val_labels=val_states,
            test_features=test_features,
            test_labels=test_states
        )
        
        # 6. è®­ç»ƒ LSTM
        logger.info("æ­¥éª¤ 6/6: è®­ç»ƒ LSTM æ¨¡å‹...")
        model_path = self.config.get_model_path(symbol)
        
        # ä½¿ç”¨éªŒè¯é›†è¿›è¡Œæ—©åœå’Œæ¨¡å‹é€‰æ‹©
        history = lstm_classifier.train(
            X_train, y_train,
            X_val, y_val,  # éªŒè¯é›†ç”¨äºæ—©åœ
            epochs=self.config.EPOCHS,
            batch_size=self.config.BATCH_SIZE,
            early_stopping_patience=self.config.EARLY_STOPPING_PATIENCE,
            lr_reduce_patience=self.config.LR_REDUCE_PATIENCE,
            model_path=model_path,
            use_class_weight=self.config.USE_CLASS_WEIGHT
        )
        
        # åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆè¿™æ‰æ˜¯çœŸå®çš„æ³›åŒ–æ€§èƒ½ï¼‰
        logger.info("åœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹...")
        if X_test is not None and y_test is not None:
            eval_results = lstm_classifier.evaluate(X_test, y_test)
            logger.info(f"ğŸ¯ æµ‹è¯•é›†å‡†ç¡®ç‡: {eval_results['accuracy']:.4f} (è¿™æ˜¯çœŸå®çš„æ³›åŒ–æ€§èƒ½)")
        else:
            # å¦‚æœæ²¡æœ‰æµ‹è¯•é›†ï¼Œä½¿ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆä¸æ¨èï¼‰
            eval_results = lstm_classifier.evaluate(X_val, y_val)
            logger.warning("âš ï¸ æ²¡æœ‰ç‹¬ç«‹æµ‹è¯•é›†ï¼Œä½¿ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆç»“æœå¯èƒ½åä¹è§‚ï¼‰")
        
        # åŒæ—¶è¾“å‡ºéªŒè¯é›†å‡†ç¡®ç‡ä½œä¸ºå‚è€ƒ
        val_eval = lstm_classifier.evaluate(X_val, y_val)
        logger.info(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_eval['accuracy']:.4f}")
        
        # ä¿å­˜æ¨¡å‹å’Œæ ‡å‡†åŒ–å™¨
        scaler_path = self.config.get_scaler_path(symbol)
        lstm_classifier.save(model_path, scaler_path)
        
        logger.info(f"å®Œæ•´é‡è®­å®Œæˆ: {symbol}")
        logger.info(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {eval_results['accuracy']:.4f}")
        
        return {
            'symbol': symbol,
            'training_type': 'full_retrain',
            'timestamp': datetime.now(),
            'test_accuracy': eval_results['accuracy'],
            'val_accuracy': val_eval['accuracy'],
            'test_loss': eval_results['loss'],
            'regime_analysis': regime_analysis,
            'history': history,
            'data_split': {
                'train_samples': len(train_features),
                'val_samples': len(val_features),
                'test_samples': len(test_features)
            }
        }
    
    def incremental_train(self, symbol: str) -> Dict:
        """
        å¢é‡è®­ç»ƒï¼ˆåœ¨ç°æœ‰æ¨¡å‹åŸºç¡€ä¸Šï¼‰
        
        Args:
            symbol: äº¤æ˜“å¯¹
            
        Returns:
            è®­ç»ƒç»“æœ
        """
        logger.info(f"="*80)
        logger.info(f"å¼€å§‹å¢é‡è®­ç»ƒ: {symbol}")
        logger.info(f"="*80)
        
        # 1. è·å–æœ€æ–°æ•°æ®
        logger.info("æ­¥éª¤ 1/4: è·å–æœ€æ–°æ•°æ®...")
        data = self.data_fetcher.fetch_latest_data(
            symbol=symbol,
            timeframes=self.config.TIMEFRAMES,
            days=self.config.INCREMENTAL_TRAIN_DAYS
        )
        
        # è¾“å‡º API ç»Ÿè®¡ä¿¡æ¯
        stats = self.data_fetcher.get_api_stats()
        logger.info(f"API è¯·æ±‚ç»Ÿè®¡: {stats}")
        
        # 2. ç‰¹å¾å·¥ç¨‹
        logger.info("æ­¥éª¤ 2/4: è®¡ç®—æŠ€æœ¯æŒ‡æ ‡...")
        features = self.feature_engineer.combine_timeframe_features(
            data,
            primary_timeframe=self.config.PRIMARY_TIMEFRAME,
            symbol=symbol
        )
        
        # 3. åŠ è½½ HMM æ¨¡å‹å¹¶æ ‡æ³¨
        logger.info("æ­¥éª¤ 3/4: HMM çŠ¶æ€æ ‡æ³¨...")
        hmm_path = self.config.get_hmm_path(symbol)
        
        if not os.path.exists(hmm_path):
            logger.warning(f"HMM æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†æ‰§è¡Œå®Œæ•´é‡è®­: {hmm_path}")
            return self.full_retrain(symbol)
        
        hmm_labeler = HMMRegimeLabeler.load(hmm_path)
        
        # åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆä¸å®Œæ•´è®­ç»ƒä¿æŒä¸€è‡´ï¼‰
        features_before_selection = features.copy()
        features = self.feature_engineer.select_key_features(features)
        
        # å¦‚æœæ¨¡å‹ä¿å­˜äº†ç‰¹å¾åç§°ï¼Œç¡®ä¿ç‰¹å¾ä¸€è‡´
        if hmm_labeler.feature_names_ is not None:
            # æ£€æŸ¥ç‰¹å¾æ˜¯å¦åŒ¹é…
            missing_features = set(hmm_labeler.feature_names_) - set(features.columns)
            extra_features = set(features.columns) - set(hmm_labeler.feature_names_)
            
            if missing_features or extra_features:
                logger.warning(
                    f"ç‰¹å¾é€‰æ‹©ç»“æœä¸ä¸€è‡´ï¼\n"
                    f"  è®­ç»ƒæ—¶ç‰¹å¾æ•°: {len(hmm_labeler.feature_names_)}\n"
                    f"  å½“å‰ç‰¹å¾æ•°: {len(features.columns)}\n"
                    f"  ç¼ºå°‘ç‰¹å¾: {len(missing_features)} ä¸ª\n"
                    f"  å¤šä½™ç‰¹å¾: {len(extra_features)} ä¸ª"
                )
                # predict æ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç†ç‰¹å¾å¯¹é½
        else:
            # æ—§ç‰ˆæœ¬æ¨¡å‹ï¼šæ£€æŸ¥ç‰¹å¾æ•°é‡
            expected_features = (
                hmm_labeler.scaler.n_features_in_ 
                if hasattr(hmm_labeler.scaler, 'n_features_in_') 
                else None
            )
            if expected_features and len(features.columns) != expected_features:
                logger.error(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼è®­ç»ƒæ—¶: {expected_features} ä¸ªç‰¹å¾, "
                    f"å½“å‰: {len(features.columns)} ä¸ªç‰¹å¾\n"
                    f"è¿™æ˜¯æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ä¿å­˜ç‰¹å¾åç§°ã€‚"
                )
                raise ValueError(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ã€‚è¯·é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§ã€‚"
                )
        
        states = hmm_labeler.predict(features)
        
        # 4. åŠ è½½ LSTM æ¨¡å‹å¹¶å¢é‡è®­ç»ƒ
        logger.info("æ­¥éª¤ 4/4: LSTM å¢é‡è®­ç»ƒ...")
        model_path = self.config.get_model_path(symbol)
        scaler_path = self.config.get_scaler_path(symbol)
        
        if not os.path.exists(model_path):
            logger.warning(f"LSTM æ¨¡å‹ä¸å­˜åœ¨ï¼Œå°†æ‰§è¡Œå®Œæ•´é‡è®­: {model_path}")
            return self.full_retrain(symbol)
        
        lstm_classifier = LSTMRegimeClassifier.load(model_path, scaler_path)
        
        # å¯¹é½ç‰¹å¾ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        # ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„ç‰¹å¾åç§°ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ scaler çš„ feature_names_in_
        feature_names = lstm_classifier.feature_names_
        if feature_names is None and hasattr(lstm_classifier.scaler, 'feature_names_in_'):
            feature_names = list(lstm_classifier.scaler.feature_names_in_)
            logger.info(f"ä½¿ç”¨ scaler çš„ç‰¹å¾åç§°: {len(feature_names)} ä¸ªç‰¹å¾")
        
        if feature_names is not None:
            # ç¡®ä¿ç‰¹å¾é¡ºåºå’Œæ•°é‡ä¸è®­ç»ƒæ—¶ä¸€è‡´
            missing_features = set(feature_names) - set(features.columns)
            extra_features = set(features.columns) - set(feature_names)
            
            if missing_features or extra_features:
                logger.warning(
                    f"ç‰¹å¾ä¸ä¸€è‡´ï¼\n"
                    f"  è®­ç»ƒæ—¶ç‰¹å¾æ•°: {len(feature_names)}\n"
                    f"  å½“å‰ç‰¹å¾æ•°: {len(features.columns)}\n"
                    f"  ç¼ºå°‘ç‰¹å¾: {len(missing_features)} ä¸ª\n"
                    f"  å¤šä½™ç‰¹å¾: {len(extra_features)} ä¸ª"
                )
                if missing_features:
                    logger.warning(f"  ç¼ºå°‘çš„ç‰¹å¾: {missing_features}")
                if extra_features:
                    logger.warning(f"  å¤šä½™çš„ç‰¹å¾: {extra_features}")
                
                # å¯¹é½ç‰¹å¾ï¼šæ·»åŠ ç¼ºå¤±çš„ç‰¹å¾ï¼ˆå¡«å……0ï¼‰ï¼Œç§»é™¤å¤šä½™çš„ç‰¹å¾
                features_aligned = features.reindex(columns=feature_names, fill_value=0)
                logger.info(f"ç‰¹å¾å·²å¯¹é½: {len(features_aligned.columns)} ä¸ªç‰¹å¾")
            else:
                # ç‰¹å¾åç§°ä¸€è‡´ï¼Œä½†éœ€è¦ç¡®ä¿é¡ºåºä¸€è‡´
                features_aligned = features[feature_names]
        else:
            # æ—§ç‰ˆæœ¬æ¨¡å‹ï¼šåªæ£€æŸ¥ç‰¹å¾æ•°é‡
            expected_features = (
                lstm_classifier.scaler.n_features_in_ 
                if hasattr(lstm_classifier.scaler, 'n_features_in_') 
                else None
            )
            if expected_features and len(features.columns) != expected_features:
                logger.error(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼è®­ç»ƒæ—¶: {expected_features} ä¸ªç‰¹å¾, "
                    f"å½“å‰: {len(features.columns)} ä¸ªç‰¹å¾\n"
                    f"è¿™æ˜¯æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œå»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ä¿å­˜ç‰¹å¾åç§°ã€‚"
                )
                raise ValueError(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ã€‚è¯·é‡æ–°è®­ç»ƒæ¨¡å‹ï¼ˆè¿è¡Œç¤ºä¾‹ 1ï¼‰ä»¥ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§ã€‚"
                )
            features_aligned = features
        
        # å‡†å¤‡å¢é‡è®­ç»ƒæ•°æ®
        features_scaled = lstm_classifier.scaler.transform(features_aligned)
        
        X, y = [], []
        for i in range(len(features_scaled) - lstm_classifier.sequence_length):
            X.append(features_scaled[i:i+lstm_classifier.sequence_length])
            y.append(states[i+lstm_classifier.sequence_length])
        
        X = np.array(X)
        y = np.array(y)
        
        # æ‰§è¡Œå¢é‡è®­ç»ƒï¼ˆå¸¦éªŒè¯é›†å’Œæ—©åœï¼‰
        lstm_classifier.incremental_train(
            X, y,
            epochs=self.config.INCREMENTAL_EPOCHS,
            batch_size=self.config.BATCH_SIZE,
            learning_rate=self.config.INCREMENTAL_LEARNING_RATE,
            validation_split=self.config.INCREMENTAL_VALIDATION_SPLIT,
            early_stopping_patience=self.config.INCREMENTAL_EARLY_STOPPING_PATIENCE,
            use_class_weight=self.config.USE_CLASS_WEIGHT
        )
        
        # ä¿å­˜æ›´æ–°åçš„æ¨¡å‹
        lstm_classifier.save(model_path, scaler_path)
        
        logger.info(f"å¢é‡è®­ç»ƒå®Œæˆ: {symbol}")
        
        return {
            'symbol': symbol,
            'training_type': 'incremental',
            'timestamp': datetime.now(),
            'samples_used': len(X)
        }
    
    def train_all_symbols(self, training_type: str = 'full') -> Dict:
        """
        è®­ç»ƒæ‰€æœ‰äº¤æ˜“å¯¹
        
        Args:
            training_type: 'full' æˆ– 'incremental'
            
        Returns:
            æ‰€æœ‰äº¤æ˜“å¯¹çš„è®­ç»ƒç»“æœ
        """
        results = {}
        
        for symbol in self.config.SYMBOLS:
            try:
                if training_type == 'full':
                    result = self.full_retrain(symbol)
                else:
                    result = self.incremental_train(symbol)
                
                results[symbol] = result
                
            except Exception as e:
                logger.error(f"è®­ç»ƒ {symbol} æ—¶å‡ºé”™: {e}", exc_info=True)
                results[symbol] = {'error': str(e)}
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    TrainingConfig.ensure_dirs()
    
    # åˆ›å»ºè®­ç»ƒç®¡é“
    pipeline = TrainingPipeline(TrainingConfig)
    
    # ç¤ºä¾‹ï¼šå®Œæ•´é‡è®­ BTC
    result = pipeline.full_retrain("BTCUSDT")
    
    logger.info(f"\nè®­ç»ƒç»“æœ: {result}")


if __name__ == "__main__":
    main()
