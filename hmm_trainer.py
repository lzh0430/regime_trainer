"""
HMM è®­ç»ƒæ¨¡å— - ç”¨äºæ— ç›‘ç£å¸‚åœºçŠ¶æ€æ ‡æ³¨

ä¿®å¤æ•°æ®æ³„æ¼é—®é¢˜ï¼š
- fit() åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ scaler, PCA, HMM
- predict() ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹ï¼ˆä¸æ³„æ¼æœªæ¥ä¿¡æ¯ï¼‰

è‡ªåŠ¨æ˜ å°„åŠŸèƒ½ï¼š
- auto_map_regimes() æ ¹æ®ç‰¹å¾ç»Ÿè®¡è‡ªåŠ¨å°† HMM çŠ¶æ€æ˜ å°„åˆ°è¯­ä¹‰åç§°
- è§£å†³äº† HMM çŠ¶æ€ç¼–å·ä»»æ„æ€§çš„é—®é¢˜
"""
import numpy as np
import pandas as pd
from hmmlearn import hmm
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import pickle
import logging
from typing import Tuple, Optional, Dict, List

logger = logging.getLogger(__name__)

# é»˜è®¤çš„ regime åç§°ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
DEFAULT_REGIME_NAMES = [
    "Strong_Trend",      # é«˜ ADX, é«˜è¶‹åŠ¿å¼ºåº¦
    "Weak_Trend",        # ä¸­ç­‰ ADX, æœ‰ä¸€å®šè¶‹åŠ¿
    "Range",             # ä½ ADX, ä¸­ç­‰æ³¢åŠ¨ç‡
    "Choppy_High_Vol",   # ä½ ADX, é«˜æ³¢åŠ¨ç‡
    "Volatility_Spike",  # æé«˜æ³¢åŠ¨ç‡
    "Squeeze",           # æä½æ³¢åŠ¨ç‡, ä½ ADX
]

class HMMRegimeLabeler:
    """HMM å¸‚åœºçŠ¶æ€æ ‡æ³¨å™¨"""
    
    def __init__(self, n_states: int = 6, n_components: int = 4, primary_timeframe: str = "15m"):
        """
        åˆå§‹åŒ–
        
        Args:
            n_states: éšè—çŠ¶æ€æ•°é‡ï¼ˆå¸‚åœºçŠ¶æ€æ•°ï¼‰
            n_components: PCA é™ç»´åçš„ç‰¹å¾æ•°
            primary_timeframe: ä¸»æ—¶é—´æ¡†æ¶ï¼ˆç”¨äºä¼˜å…ˆåŒ¹é…ç‰¹å¾åˆ—ï¼‰
        """
        self.n_states = n_states
        self.n_components = n_components
        self.primary_timeframe = primary_timeframe
        self.hmm_model = None
        self.pca = None
        self.scaler = None
        self.feature_names_ = None  # ä¿å­˜è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾åç§°
        self.regime_mapping_ = None  # çŠ¶æ€ ID åˆ°è¯­ä¹‰åç§°çš„æ˜ å°„ {state_id: regime_name}
        self.state_profiles_ = None  # æ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾ profileï¼ˆç”¨äºå®¡è®¡ï¼‰
        self.training_bic_ = None  # è®­ç»ƒæ—¶çš„ BIC å€¼
        self.transition_matrix_ = None  # çŠ¶æ€è½¬ç§»çŸ©é˜µ
    
    def fit(self, features: pd.DataFrame, n_iter: int = 100) -> np.ndarray:
        """
        è®­ç»ƒ HMM å¹¶æ ‡æ³¨å¸‚åœºçŠ¶æ€
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•åªåº”åœ¨è®­ç»ƒé›†ä¸Šè°ƒç”¨ï¼Œé¿å…æ•°æ®æ³„æ¼ã€‚
        å¯¹äºéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œåº”ä½¿ç”¨ predict() æ–¹æ³•ã€‚
        
        Args:
            features: ç‰¹å¾ DataFrameï¼ˆåº”åªåŒ…å«è®­ç»ƒé›†æ•°æ®ï¼‰
            n_iter: HMM è®­ç»ƒè¿­ä»£æ¬¡æ•°
            
        Returns:
            çŠ¶æ€æ ‡ç­¾æ•°ç»„ï¼ˆè®­ç»ƒé›†çš„æ ‡ç­¾ï¼‰
        """
        logger.info(f"å¼€å§‹ HMM è®­ç»ƒï¼Œç‰¹å¾ç»´åº¦: {features.shape}")
        logger.info("æ³¨æ„ï¼šHMM åªåœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆï¼Œé¿å…æ•°æ®æ³„æ¼")
        
        # ä¿å­˜ç‰¹å¾åç§°ï¼ˆç”¨äºåç»­é¢„æµ‹æ—¶ç¡®ä¿ç‰¹å¾ä¸€è‡´ï¼‰
        self.feature_names_ = list(features.columns)
        
        # 1. æ ‡å‡†åŒ–ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Š fitï¼‰
        self.scaler = StandardScaler()
        features_scaled = self.scaler.fit_transform(features)
        
        # 2. PCA é™ç»´ï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Š fitï¼‰
        self.pca = PCA(n_components=self.n_components)
        features_pca = self.pca.fit_transform(features_scaled)
        
        logger.info(f"PCA è§£é‡Šæ–¹å·®æ¯”: {self.pca.explained_variance_ratio_}")
        logger.info(f"PCA ç´¯è®¡è§£é‡Šæ–¹å·®: {np.cumsum(self.pca.explained_variance_ratio_)}")
        
        # 3. è®­ç»ƒ HMMï¼ˆåªåœ¨è®­ç»ƒé›†ä¸Š fitï¼‰
        self.hmm_model = hmm.GaussianHMM(
            n_components=self.n_states,
            covariance_type="full",
            n_iter=n_iter,
            random_state=42,
            verbose=True
        )
        
        self.hmm_model.fit(features_pca)
        
        # 4. é¢„æµ‹è®­ç»ƒé›†çš„çŠ¶æ€
        states = self.hmm_model.predict(features_pca)
        
        # 5. ä¿å­˜ BIC å€¼å’Œè½¬ç§»çŸ©é˜µ
        self.training_bic_ = self.hmm_model.bic(features_pca)
        self.compute_transition_matrix(states)
        
        logger.info(f"HMM è®­ç»ƒå®Œæˆï¼ŒBIC: {self.training_bic_:.2f}")
        logger.info(f"è®­ç»ƒé›†çŠ¶æ€åˆ†å¸ƒ: {np.bincount(states, minlength=self.n_states).tolist()}")
        logger.debug(
            "æ³¨æ„ï¼šHMM åœ¨è®­ç»ƒé›†ä¸Š fitï¼Œä¼šè‡ªåŠ¨å°†æ•°æ®åˆ†é…åˆ° 6 ä¸ªçŠ¶æ€ï¼Œå› æ­¤è®­ç»ƒé›†ä¸€å®šæœ‰æ‰€æœ‰çŠ¶æ€ã€‚"
            "éªŒè¯/æµ‹è¯•é›†ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ predictï¼Œå¦‚æœé‚£æ®µæ—¶é—´å¸‚åœºæ²¡æœ‰æŸç§çŠ¶æ€ï¼Œå°±ä¸ä¼šå‡ºç°è¯¥çŠ¶æ€ã€‚"
        )
        
        return states
    
    def fit_predict_split(
        self, 
        train_features: pd.DataFrame, 
        val_features: Optional[pd.DataFrame] = None,
        test_features: Optional[pd.DataFrame] = None,
        n_iter: int = 100
    ) -> Tuple[np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]:
        """
        åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ HMMï¼Œå¹¶åˆ†åˆ«é¢„æµ‹è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†çš„çŠ¶æ€æ ‡ç­¾
        
        è¿™æ˜¯æ¨èçš„æ–¹æ³•ï¼Œå¯ä»¥é¿å…æ•°æ®æ³„æ¼ï¼š
        - Scaler åªåœ¨è®­ç»ƒé›†ä¸Š fit
        - PCA åªåœ¨è®­ç»ƒé›†ä¸Š fit  
        - HMM åªåœ¨è®­ç»ƒé›†ä¸Š fit
        - éªŒè¯é›†å’Œæµ‹è¯•é›†åªåš transform å’Œ predict
        
        Args:
            train_features: è®­ç»ƒé›†ç‰¹å¾
            val_features: éªŒè¯é›†ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            test_features: æµ‹è¯•é›†ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            n_iter: HMM è®­ç»ƒè¿­ä»£æ¬¡æ•°
            
        Returns:
            (train_states, val_states, test_states) - å„æ•°æ®é›†çš„çŠ¶æ€æ ‡ç­¾
        """
        # åœ¨è®­ç»ƒé›†ä¸Šæ‹Ÿåˆ
        train_states = self.fit(train_features, n_iter=n_iter)
        
        # é¢„æµ‹éªŒè¯é›†çŠ¶æ€ï¼ˆä½¿ç”¨è®­ç»ƒé›†æ‹Ÿåˆçš„ scaler/PCA/HMMï¼‰
        val_states = None
        if val_features is not None and len(val_features) > 0:
            val_states = self.predict(val_features)
            logger.info(f"éªŒè¯é›†çŠ¶æ€åˆ†å¸ƒ: {np.bincount(val_states, minlength=self.n_states).tolist()}")
        
        # é¢„æµ‹æµ‹è¯•é›†çŠ¶æ€
        test_states = None
        if test_features is not None and len(test_features) > 0:
            test_states = self.predict(test_features)
            logger.info(f"æµ‹è¯•é›†çŠ¶æ€åˆ†å¸ƒ: {np.bincount(test_states, minlength=self.n_states).tolist()}")
        
        return train_states, val_states, test_states
    
    def predict(self, features: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹å¸‚åœºçŠ¶æ€
        
        Args:
            features: ç‰¹å¾ DataFrame
            
        Returns:
            çŠ¶æ€æ ‡ç­¾æ•°ç»„
        """
        if self.hmm_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ fit()")
        
        # ç¡®ä¿ç‰¹å¾åˆ—ä¸è®­ç»ƒæ—¶ä¸€è‡´
        if self.feature_names_ is not None:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„ç‰¹å¾
            missing_features = set(self.feature_names_) - set(features.columns)
            if missing_features:
                # å°è¯•æ·»åŠ ç¼ºå¤±çš„ç‰¹å¾ï¼ˆå¡«å……ä¸º0ï¼‰
                logger.warning(
                    f"ç¼ºå°‘ {len(missing_features)} ä¸ªè®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾ï¼Œå°†å¡«å……ä¸º0: "
                    f"{list(missing_features)[:5]}..."
                )
                for feat in missing_features:
                    features[feat] = 0.0
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é¢å¤–çš„ç‰¹å¾
            extra_features = set(features.columns) - set(self.feature_names_)
            if extra_features:
                logger.debug(f"ç§»é™¤ {len(extra_features)} ä¸ªè®­ç»ƒæ—¶æœªä½¿ç”¨çš„ç‰¹å¾")
            
            # åªé€‰æ‹©è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾ï¼Œå¹¶æŒ‰ç…§è®­ç»ƒæ—¶çš„é¡ºåºæ’åˆ—
            features = features[self.feature_names_]
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰ä¿å­˜ç‰¹å¾åç§°ï¼Œæ£€æŸ¥ç‰¹å¾æ•°é‡
            expected_features = self.scaler.n_features_in_ if hasattr(self.scaler, 'n_features_in_') else None
            if expected_features and len(features.columns) != expected_features:
                raise ValueError(
                    f"ç‰¹å¾æ•°é‡ä¸åŒ¹é…ï¼è®­ç»ƒæ—¶: {expected_features} ä¸ªç‰¹å¾, "
                    f"å½“å‰: {len(features.columns)} ä¸ªç‰¹å¾\n"
                    f"è¿™æ˜¯æ—§ç‰ˆæœ¬æ¨¡å‹ï¼Œè¯·é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥ä¿å­˜ç‰¹å¾åç§°ã€‚"
                )
            logger.warning(
                "æ¨¡å‹ä¸­æ²¡æœ‰ä¿å­˜ç‰¹å¾åç§°ï¼ˆæ—§ç‰ˆæœ¬æ¨¡å‹ï¼‰ã€‚"
                "å»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥ç¡®ä¿ç‰¹å¾ä¸€è‡´æ€§ã€‚"
            )
        
        # åº”ç”¨ç›¸åŒçš„é¢„å¤„ç†
        features_scaled = self.scaler.transform(features)
        features_pca = self.pca.transform(features_scaled)
        
        states = self.hmm_model.predict(features_pca)
        return states
    
    def predict_proba(self, features: pd.DataFrame) -> np.ndarray:
        """
        é¢„æµ‹å¸‚åœºçŠ¶æ€çš„æ¦‚ç‡åˆ†å¸ƒ
        
        Args:
            features: ç‰¹å¾ DataFrame
            
        Returns:
            çŠ¶æ€æ¦‚ç‡çŸ©é˜µ (n_samples, n_states)
        """
        if self.hmm_model is None:
            raise ValueError("æ¨¡å‹å°šæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ fit()")
        
        features_scaled = self.scaler.transform(features)
        features_pca = self.pca.transform(features_scaled)
        
        # è®¡ç®—åéªŒæ¦‚ç‡
        log_prob, posteriors = self.hmm_model.score_samples(features_pca)
        
        return posteriors
    
    def save(self, filepath: str):
        """ä¿å­˜æ¨¡å‹ï¼ˆåŒ…æ‹¬çŠ¶æ€æ˜ å°„ã€ç‰¹å¾ profileã€BIC ç­‰å®Œæ•´ä¿¡æ¯ï¼‰"""
        model_data = {
            'hmm_model': self.hmm_model,
            'pca': self.pca,
            'scaler': self.scaler,
            'n_states': self.n_states,
            'n_components': self.n_components,
            'primary_timeframe': self.primary_timeframe,  # ä¿å­˜ä¸»æ—¶é—´æ¡†æ¶
            'feature_names': self.feature_names_,  # ä¿å­˜ç‰¹å¾åç§°
            'regime_mapping': self.regime_mapping_,  # ä¿å­˜çŠ¶æ€åˆ°è¯­ä¹‰åç§°çš„æ˜ å°„
            'state_profiles': self.state_profiles_,  # ä¿å­˜ç‰¹å¾ profileï¼ˆç”¨äºå®¡è®¡ï¼‰
            'training_bic': self.training_bic_,  # ä¿å­˜è®­ç»ƒæ—¶çš„ BIC å€¼
            'transition_matrix': self.transition_matrix_,  # ä¿å­˜è½¬ç§»çŸ©é˜µ
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        
        logger.info(f"HMM æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")
        if self.regime_mapping_:
            logger.info(f"å·²ä¿å­˜çš„çŠ¶æ€æ˜ å°„: {self.regime_mapping_}")
        if self.training_bic_:
            logger.info(f"å·²ä¿å­˜çš„ BIC å€¼: {self.training_bic_:.2f}")
    
    @classmethod
    def load(cls, filepath: str) -> 'HMMRegimeLabeler':
        """åŠ è½½æ¨¡å‹ï¼ˆåŒ…æ‹¬çŠ¶æ€æ˜ å°„ã€ç‰¹å¾ profileã€BIC ç­‰å®Œæ•´ä¿¡æ¯ï¼‰"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        
        labeler = cls(
            n_states=model_data['n_states'],
            n_components=model_data['n_components'],
            primary_timeframe=model_data.get('primary_timeframe', '15m')  # å‘åå…¼å®¹
        )
        labeler.hmm_model = model_data['hmm_model']
        labeler.pca = model_data['pca']
        labeler.scaler = model_data['scaler']
        labeler.feature_names_ = model_data.get('feature_names')  # åŠ è½½ç‰¹å¾åç§°ï¼ˆå‘åå…¼å®¹ï¼‰
        labeler.regime_mapping_ = model_data.get('regime_mapping')  # åŠ è½½çŠ¶æ€æ˜ å°„ï¼ˆå‘åå…¼å®¹ï¼‰
        labeler.state_profiles_ = model_data.get('state_profiles')  # åŠ è½½ç‰¹å¾ profile
        labeler.training_bic_ = model_data.get('training_bic')  # åŠ è½½ BIC å€¼
        labeler.transition_matrix_ = model_data.get('transition_matrix')  # åŠ è½½è½¬ç§»çŸ©é˜µ
        
        logger.info(f"HMM æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        if labeler.feature_names_:
            logger.info(f"è®­ç»ƒæ—¶ä½¿ç”¨çš„ç‰¹å¾æ•°: {len(labeler.feature_names_)}")
        if labeler.regime_mapping_:
            logger.info(f"å·²åŠ è½½çš„çŠ¶æ€æ˜ å°„: {labeler.regime_mapping_}")
        else:
            logger.warning(
                "æ¨¡å‹ä¸­æ²¡æœ‰ä¿å­˜çŠ¶æ€æ˜ å°„ï¼ˆæ—§ç‰ˆæœ¬æ¨¡å‹ï¼‰ã€‚"
                "å»ºè®®é‡æ–°è®­ç»ƒæ¨¡å‹ä»¥ç”ŸæˆçŠ¶æ€æ˜ å°„ã€‚"
            )
        if labeler.training_bic_:
            logger.info(f"è®­ç»ƒæ—¶çš„ BIC å€¼: {labeler.training_bic_:.2f}")
        return labeler
    
    def analyze_regimes(self, features: pd.DataFrame, states: np.ndarray) -> pd.DataFrame:
        """
        åˆ†æä¸åŒå¸‚åœºçŠ¶æ€çš„ç‰¹å¾
        
        Args:
            features: ç‰¹å¾ DataFrame
            states: çŠ¶æ€æ ‡ç­¾
            
        Returns:
            æ¯ä¸ªçŠ¶æ€çš„ç»Ÿè®¡ä¿¡æ¯
        """
        regime_stats = []
        
        for state in range(self.n_states):
            mask = states == state
            state_features = features[mask]
            
            stats = {
                'state': state,
                'count': mask.sum(),
                'percentage': mask.sum() / len(states) * 100,
            }
            
            # è®¡ç®—ä¸€äº›å…³é”®æŒ‡æ ‡çš„å¹³å‡å€¼
            if len(state_features) > 0:
                for col in features.columns:
                    if 'returns' in col.lower():
                        stats[f'{col}_mean'] = state_features[col].mean()
                    elif 'atr' in col.lower() or 'volatility' in col.lower():
                        stats[f'{col}_mean'] = state_features[col].mean()
                    elif 'adx' in col.lower():
                        stats[f'{col}_mean'] = state_features[col].mean()
            
            regime_stats.append(stats)
        
        return pd.DataFrame(regime_stats)
    
    # ==================== è‡ªåŠ¨æ˜ å°„åŠŸèƒ½ ====================
    
    def _find_feature_column(self, features: pd.DataFrame, pattern: str) -> Optional[str]:
        """
        æ ¹æ®æ¨¡å¼æŸ¥æ‰¾ç‰¹å¾åˆ—åï¼ˆä¼˜å…ˆåŒ¹é… primary timeframeï¼‰
        
        å½“å­˜åœ¨å¤šä¸ªæ—¶é—´æ¡†æ¶çš„ç‰¹å¾ï¼ˆå¦‚ 5m_adx, 15m_adx, 1h_adxï¼‰æ—¶ï¼Œ
        ä¼˜å…ˆè¿”å› primary timeframe çš„åˆ—ï¼Œé¿å…éšæœºæ‹¾å–å¯¼è‡´å‘½åä¸ç¨³å®šã€‚
        
        Args:
            features: ç‰¹å¾ DataFrame
            pattern: è¦åŒ¹é…çš„æ¨¡å¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
            
        Returns:
            åŒ¹é…çš„åˆ—åï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å› None
        """
        matching_cols = []
        for col in features.columns:
            if pattern.lower() in col.lower():
                matching_cols.append(col)
        
        if not matching_cols:
            return None
        
        # å¦‚æœåªæœ‰ä¸€ä¸ªåŒ¹é…ï¼Œç›´æ¥è¿”å›
        if len(matching_cols) == 1:
            return matching_cols[0]
        
        # ä¼˜å…ˆè¿”å› primary timeframe çš„åˆ—
        primary_tf = self.primary_timeframe.lower()
        for col in matching_cols:
            if col.lower().startswith(primary_tf):
                return col
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° primary timeframe çš„åˆ—ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›ç¬¬ä¸€ä¸ª
        logger.debug(
            f"æœªæ‰¾åˆ° {self.primary_timeframe} æ—¶é—´æ¡†æ¶çš„ '{pattern}' ç‰¹å¾ï¼Œ"
            f"ä½¿ç”¨ç¬¬ä¸€ä¸ªåŒ¹é…çš„åˆ—: {matching_cols[0]}"
        )
        return matching_cols[0]
    
    def _safe_mean(self, df: pd.DataFrame, pattern: str) -> float:
        """
        å®‰å…¨åœ°è®¡ç®—åŒ…å«æŸæ¨¡å¼çš„åˆ—çš„å‡å€¼
        
        Args:
            df: DataFrame
            pattern: åˆ—åæ¨¡å¼
            
        Returns:
            å‡å€¼ï¼Œå¦‚æœåˆ—ä¸å­˜åœ¨åˆ™è¿”å› 0.0
        """
        col = self._find_feature_column(df, pattern)
        if col is not None and col in df.columns:
            return df[col].mean()
        return 0.0
    
    def _calc_trend_strength(self, df: pd.DataFrame) -> float:
        """
        è®¡ç®—è¶‹åŠ¿å¼ºåº¦ï¼ˆåŸºäºæ”¶ç›Šçš„æ–¹å‘ä¸€è‡´æ€§ï¼‰
        
        Args:
            df: DataFrame
            
        Returns:
            è¶‹åŠ¿å¼ºåº¦å€¼
        """
        returns_col = self._find_feature_column(df, 'returns')
        if returns_col is not None and returns_col in df.columns:
            returns = df[returns_col]
            # æ–¹å‘ä¸€è‡´æ€§ï¼šç»å¯¹æ”¶ç›Šå‡å€¼ * æ–¹å‘ç¬¦å·
            if len(returns) > 0:
                return abs(returns.mean()) * 1000  # æ”¾å¤§ä»¥ä¾¿æ¯”è¾ƒ
        return 0.0
    
    def _calc_state_profile(self, features: pd.DataFrame, states: np.ndarray, state: int) -> Dict:
        """
        è®¡ç®—å•ä¸ªçŠ¶æ€çš„ç‰¹å¾ profile
        
        Args:
            features: ç‰¹å¾ DataFrame
            states: çŠ¶æ€æ ‡ç­¾æ•°ç»„
            state: è¦åˆ†æçš„çŠ¶æ€ ID
            
        Returns:
            çŠ¶æ€çš„ç‰¹å¾ profile å­—å…¸
        """
        mask = states == state
        state_features = features[mask]
        
        if len(state_features) == 0:
            return {
                'state': state,
                'count': 0,
                'adx_mean': 0.0,
                'atr_pct_mean': 0.0,
                'bb_width_mean': 0.0,
                'volatility_score': 0.0,
                'trend_strength': 0.0,
                'returns_abs_mean': 0.0,
            }
        
        # è®¡ç®—å…³é”®æŒ‡æ ‡
        adx_mean = self._safe_mean(state_features, 'adx')
        
        # æ³¢åŠ¨ç‡ç›¸å…³ï¼ˆATR ç™¾åˆ†æ¯”ã€BB å®½åº¦ï¼‰
        atr_14 = self._safe_mean(state_features, 'atr_14')
        bb_width = self._safe_mean(state_features, 'bb_width')
        hl_pct = self._safe_mean(state_features, 'hl_pct')
        
        # è®¡ç®—ç›¸å¯¹äºä»·æ ¼çš„ ATR ç™¾åˆ†æ¯”
        # æ³¨æ„ï¼šatr_14 æ˜¯ç»å¯¹å€¼ï¼Œéœ€è¦æ ‡å‡†åŒ–
        # ä½¿ç”¨ hl_pct ä½œä¸ºæ³¢åŠ¨ç‡çš„ä»£ç†æŒ‡æ ‡
        volatility_score = hl_pct if hl_pct > 0 else bb_width
        
        # è¶‹åŠ¿å¼ºåº¦
        trend_strength = self._calc_trend_strength(state_features)
        
        # ç»å¯¹æ”¶ç›Šå‡å€¼
        returns_col = self._find_feature_column(state_features, 'returns')
        returns_abs_mean = 0.0
        if returns_col is not None:
            returns_abs_mean = state_features[returns_col].abs().mean()
        
        return {
            'state': state,
            'count': mask.sum(),
            'adx_mean': adx_mean,
            'atr_pct_mean': hl_pct,  # ä½¿ç”¨ hl_pct ä½œä¸ºæ³¢åŠ¨ç‡æŒ‡æ ‡
            'bb_width_mean': bb_width,
            'volatility_score': volatility_score,
            'trend_strength': trend_strength,
            'returns_abs_mean': returns_abs_mean,
        }
    
    def _select_best_fallback_name(
        self, 
        profile: Dict, 
        available_names: set, 
        adx_median: float, 
        vol_median: float
    ) -> str:
        """
        æ ¹æ®çŠ¶æ€ç‰¹å¾é€‰æ‹©æœ€åˆé€‚çš„ fallback åç§°
        
        ä¸æ˜¯éšæœºé€‰æ‹©ï¼Œè€Œæ˜¯æ ¹æ®ç‰¹å¾ä¸å„ regime çš„å…¸å‹ç‰¹å¾è¿›è¡ŒåŒ¹é…ã€‚
        
        å…¸å‹ç‰¹å¾ï¼š
        - Strong_Trend: é«˜ ADX (> median), é«˜è¶‹åŠ¿å¼ºåº¦
        - Weak_Trend: ä¸­ç­‰ ADX
        - Range: ä½ ADX, ä¸­ç­‰æ³¢åŠ¨ç‡
        - Choppy_High_Vol: ä½ ADX, é«˜æ³¢åŠ¨ç‡
        - Volatility_Spike: æé«˜æ³¢åŠ¨ç‡
        - Squeeze: æä½æ³¢åŠ¨ç‡, ä½ ADX
        
        Args:
            profile: çŠ¶æ€ç‰¹å¾ profile
            available_names: å¯ç”¨çš„ regime åç§°é›†åˆ
            adx_median: ADX ä¸­ä½æ•°
            vol_median: æ³¢åŠ¨ç‡ä¸­ä½æ•°
            
        Returns:
            æœ€åˆé€‚çš„ regime åç§°
        """
        adx = profile['adx_mean']
        vol = profile['volatility_score']
        trend = profile['trend_strength']
        
        # è®¡ç®—æ¯ä¸ªå¯ç”¨åç§°çš„åŒ¹é…åˆ†æ•°ï¼ˆè¶Šé«˜è¶ŠåŒ¹é…ï¼‰
        scores = {}
        
        for name in available_names:
            score = 0
            
            if name == 'Strong_Trend':
                # é«˜ ADX + é«˜è¶‹åŠ¿å¼ºåº¦
                score = (adx / adx_median) + (trend * 10)
                
            elif name == 'Weak_Trend':
                # ä¸­ç­‰ ADX
                if adx_median * 0.6 < adx < adx_median * 1.4:
                    score = 1.0 - abs(adx - adx_median) / adx_median
                else:
                    score = 0.1
                    
            elif name == 'Range':
                # ä½ ADX + ä¸­ç­‰æ³¢åŠ¨ç‡
                if adx < adx_median:
                    score = (1 - adx / adx_median) * 0.5
                    if vol_median * 0.5 < vol < vol_median * 1.5:
                        score += 0.5
                        
            elif name == 'Choppy_High_Vol':
                # ä½ ADX + é«˜æ³¢åŠ¨ç‡
                if adx < adx_median and vol > vol_median:
                    score = (vol / vol_median) * (1 - adx / adx_median)
                    
            elif name == 'Volatility_Spike':
                # æé«˜æ³¢åŠ¨ç‡
                score = vol / vol_median if vol_median > 0 else 0
                
            elif name == 'Squeeze':
                # æä½æ³¢åŠ¨ç‡ + ä½ ADX
                if vol < vol_median and adx < adx_median:
                    score = (1 - vol / vol_median) * (1 - adx / adx_median)
            
            scores[name] = score
        
        # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„åç§°
        best_name = max(scores, key=scores.get)
        
        logger.debug(
            f"Fallback åç§°é€‰æ‹©: ADX={adx:.2f}, vol={vol:.4f}, "
            f"scores={scores}, best={best_name}"
        )
        
        return best_name
    
    def auto_map_regimes(
        self, 
        features: pd.DataFrame, 
        states: np.ndarray,
        min_vol_for_spike: float = 0.02,
        max_vol_for_squeeze: float = 0.01,
        min_adx_for_strong_trend: float = 30,
        max_adx_for_squeeze: float = 20
    ) -> Dict[int, str]:
        """
        æ ¹æ®ç‰¹å¾ç»Ÿè®¡è‡ªåŠ¨æ˜ å°„ HMM çŠ¶æ€åˆ°è¯­ä¹‰åç§°
        
        ä½¿ç”¨**ç›¸å¯¹é˜ˆå€¼ + ç»å¯¹é˜ˆå€¼æŠ¤æ **çš„ç»„åˆåˆ¤æ–­ç­–ç•¥ï¼š
        - ç›¸å¯¹é˜ˆå€¼ï¼šåŸºäºæ‰€æœ‰çŠ¶æ€çš„ä¸­ä½æ•°å€æ•°ï¼ˆé€‚åº”ä¸åŒå¸‚åœºæ¡ä»¶ï¼‰
        - ç»å¯¹é˜ˆå€¼æŠ¤æ ï¼šé˜²æ­¢åœ¨æç«¯å¸‚åœºæ¡ä»¶ä¸‹ï¼ˆå¦‚æ‰€æœ‰çŠ¶æ€éƒ½ä½æ³¢åŠ¨ï¼‰å‡ºç°è¯¯æ ‡è®°
        
        ä¾‹å¦‚ï¼šVolatility_Spike å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
        - ç›¸å¯¹æ¡ä»¶ï¼šæ³¢åŠ¨ç‡ > ä¸­ä½æ•° * 1.5
        - ç»å¯¹æ¡ä»¶ï¼šæ³¢åŠ¨ç‡ > min_vol_for_spike (é»˜è®¤ 0.02)
        
        Args:
            features: ç‰¹å¾ DataFrame
            states: HMM é¢„æµ‹çš„çŠ¶æ€æ•°ç»„
            min_vol_for_spike: Volatility_Spike çš„æœ€å°æ³¢åŠ¨ç‡é˜ˆå€¼
            max_vol_for_squeeze: Squeeze çš„æœ€å¤§æ³¢åŠ¨ç‡é˜ˆå€¼
            min_adx_for_strong_trend: Strong_Trend çš„æœ€å° ADX é˜ˆå€¼
            max_adx_for_squeeze: Squeeze çš„æœ€å¤§ ADX é˜ˆå€¼
            
        Returns:
            {state_id: regime_name} æ˜ å°„å­—å…¸
        """
        logger.info("å¼€å§‹è‡ªåŠ¨æ˜ å°„ HMM çŠ¶æ€åˆ°è¯­ä¹‰åç§°...")
        logger.info(
            f"ç»å¯¹é˜ˆå€¼æŠ¤æ : min_vol_spike={min_vol_for_spike}, max_vol_squeeze={max_vol_for_squeeze}, "
            f"min_adx_strong={min_adx_for_strong_trend}, max_adx_squeeze={max_adx_for_squeeze}"
        )
        
        # è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„ç‰¹å¾ profile
        profiles = []
        for state in range(self.n_states):
            profile = self._calc_state_profile(features, states, state)
            profiles.append(profile)
            logger.debug(
                f"State {state}: count={profile['count']}, "
                f"ADX={profile['adx_mean']:.2f}, "
                f"volatility={profile['volatility_score']:.4f}, "
                f"trend_strength={profile['trend_strength']:.4f}"
            )
        
        # è¿‡æ»¤æ‰ç©ºçŠ¶æ€
        valid_profiles = [p for p in profiles if p['count'] > 0]
        
        if not valid_profiles:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆçš„çŠ¶æ€ï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„")
            return {i: f"State_{i}" for i in range(self.n_states)}
        
        # è®¡ç®—ç»Ÿè®¡é‡ç”¨äºå½’ä¸€åŒ–æ¯”è¾ƒï¼ˆç›¸å¯¹é˜ˆå€¼åŸºå‡†ï¼‰
        all_adx = [p['adx_mean'] for p in valid_profiles]
        all_vol = [p['volatility_score'] for p in valid_profiles]
        
        adx_median = np.median(all_adx) if all_adx else 25
        vol_median = np.median(all_vol) if all_vol else 0.01
        
        logger.info(f"ç›¸å¯¹é˜ˆå€¼åŸºå‡†: ADX_median={adx_median:.2f}, vol_median={vol_median:.4f}")
        
        # åˆ†é… regime åç§°
        mapping = {}
        used_names = set()
        
        # æŒ‰ä¼˜å…ˆçº§åˆ†é…åç§°
        # 1. Volatility_Spike: æ³¢åŠ¨ç‡æœ€é«˜çš„çŠ¶æ€
        #    æ¡ä»¶ï¼šæ³¢åŠ¨ç‡ > vol_median * 1.5 ä¸” > min_vol_for_spikeï¼ˆç»å¯¹æŠ¤æ ï¼‰
        vol_sorted = sorted(valid_profiles, key=lambda x: x['volatility_score'], reverse=True)
        if vol_sorted:
            candidate = vol_sorted[0]
            relative_ok = candidate['volatility_score'] > vol_median * 1.5
            absolute_ok = candidate['volatility_score'] > min_vol_for_spike
            
            if relative_ok and absolute_ok:
                mapping[candidate['state']] = 'Volatility_Spike'
                used_names.add('Volatility_Spike')
                logger.info(f"State {candidate['state']} -> Volatility_Spike (volatility={candidate['volatility_score']:.4f})")
            elif relative_ok and not absolute_ok:
                logger.info(
                    f"State {candidate['state']} æ»¡è¶³ç›¸å¯¹æ¡ä»¶ä½†ä¸æ»¡è¶³ç»å¯¹æŠ¤æ  "
                    f"(volatility={candidate['volatility_score']:.4f} < {min_vol_for_spike})ï¼Œä¸æ ‡è®°ä¸º Volatility_Spike"
                )
        
        # 2. Squeeze: æ³¢åŠ¨ç‡æœ€ä½ + ADX æœ€ä½
        #    æ¡ä»¶ï¼šæ³¢åŠ¨ç‡ < vol_median * 0.7 ä¸” < max_vol_for_squeezeï¼ˆç»å¯¹æŠ¤æ ï¼‰
        #          ADX < adx_median ä¸” < max_adx_for_squeezeï¼ˆç»å¯¹æŠ¤æ ï¼‰
        remaining = [p for p in valid_profiles if p['state'] not in mapping]
        if remaining:
            squeeze_sorted = sorted(remaining, key=lambda x: x['volatility_score'] + x['adx_mean'] / 100)
            candidate = squeeze_sorted[0]
            
            vol_relative_ok = candidate['volatility_score'] < vol_median * 0.7
            vol_absolute_ok = candidate['volatility_score'] < max_vol_for_squeeze
            adx_relative_ok = candidate['adx_mean'] < adx_median
            adx_absolute_ok = candidate['adx_mean'] < max_adx_for_squeeze
            
            if vol_relative_ok and vol_absolute_ok and adx_relative_ok and adx_absolute_ok:
                mapping[candidate['state']] = 'Squeeze'
                used_names.add('Squeeze')
                logger.info(f"State {candidate['state']} -> Squeeze (volatility={candidate['volatility_score']:.4f}, ADX={candidate['adx_mean']:.2f})")
            elif (vol_relative_ok or adx_relative_ok) and not (vol_absolute_ok and adx_absolute_ok):
                logger.info(
                    f"State {candidate['state']} æ»¡è¶³ç›¸å¯¹æ¡ä»¶ä½†ä¸æ»¡è¶³ç»å¯¹æŠ¤æ ï¼Œä¸æ ‡è®°ä¸º Squeeze "
                    f"(volatility={candidate['volatility_score']:.4f}, ADX={candidate['adx_mean']:.2f})"
                )
        
        # 3. Strong_Trend: ADX æœ€é«˜ + è¶‹åŠ¿å¼ºåº¦é«˜
        #    æ¡ä»¶ï¼šADX > adx_median * 1.2 ä¸” > min_adx_for_strong_trendï¼ˆç»å¯¹æŠ¤æ ï¼‰
        remaining = [p for p in valid_profiles if p['state'] not in mapping]
        if remaining:
            trend_sorted = sorted(remaining, key=lambda x: x['adx_mean'] + x['trend_strength'] * 10, reverse=True)
            candidate = trend_sorted[0]
            
            relative_ok = candidate['adx_mean'] > adx_median * 1.2
            absolute_ok = candidate['adx_mean'] > min_adx_for_strong_trend
            
            if relative_ok and absolute_ok:
                mapping[candidate['state']] = 'Strong_Trend'
                used_names.add('Strong_Trend')
                logger.info(f"State {candidate['state']} -> Strong_Trend (ADX={candidate['adx_mean']:.2f})")
            elif relative_ok and not absolute_ok:
                logger.info(
                    f"State {candidate['state']} æ»¡è¶³ç›¸å¯¹æ¡ä»¶ä½†ä¸æ»¡è¶³ç»å¯¹æŠ¤æ  "
                    f"(ADX={candidate['adx_mean']:.2f} < {min_adx_for_strong_trend})ï¼Œä¸æ ‡è®°ä¸º Strong_Trend"
                )
        
        # 4. Choppy_High_Vol: é«˜æ³¢åŠ¨ + ä½ ADXï¼ˆä¸éœ€è¦ç»å¯¹æŠ¤æ ï¼Œå·²ç”± Volatility_Spike è¿‡æ»¤ï¼‰
        remaining = [p for p in valid_profiles if p['state'] not in mapping]
        if remaining:
            choppy_sorted = sorted(remaining, key=lambda x: x['volatility_score'] - x['adx_mean'] / 100, reverse=True)
            candidate = choppy_sorted[0]
            if candidate['volatility_score'] > vol_median and candidate['adx_mean'] < adx_median:
                mapping[candidate['state']] = 'Choppy_High_Vol'
                used_names.add('Choppy_High_Vol')
                logger.info(f"State {candidate['state']} -> Choppy_High_Vol (volatility={candidate['volatility_score']:.4f}, ADX={candidate['adx_mean']:.2f})")
        
        # 5. Weak_Trend: ä¸­ç­‰ ADX
        remaining = [p for p in valid_profiles if p['state'] not in mapping]
        if remaining:
            weak_trend_sorted = sorted(remaining, key=lambda x: x['adx_mean'], reverse=True)
            candidate = weak_trend_sorted[0]
            if candidate['adx_mean'] > adx_median * 0.8:
                mapping[candidate['state']] = 'Weak_Trend'
                used_names.add('Weak_Trend')
                logger.info(f"State {candidate['state']} -> Weak_Trend (ADX={candidate['adx_mean']:.2f})")
        
        # 6. Range: å‰©ä½™çš„çŠ¶æ€
        remaining = [p for p in valid_profiles if p['state'] not in mapping]
        for p in remaining:
            if 'Range' not in used_names:
                mapping[p['state']] = 'Range'
                used_names.add('Range')
                logger.info(f"State {p['state']} -> Range")
            else:
                # å¦‚æœè¿˜æœ‰å‰©ä½™ï¼Œæ ¹æ®ç‰¹å¾é€‰æ‹©æœ€æ¥è¿‘çš„åç§°ï¼ˆä¸æ˜¯éšæœºåˆ†é…ï¼ï¼‰
                available_names = set(DEFAULT_REGIME_NAMES) - used_names
                if available_names:
                    # æ ¹æ®ç‰¹å¾é€‰æ‹©æœ€åˆé€‚çš„åç§°
                    best_name = self._select_best_fallback_name(
                        p, available_names, adx_median, vol_median
                    )
                    mapping[p['state']] = best_name
                    used_names.add(best_name)
                    logger.info(
                        f"State {p['state']} -> {best_name} (fallback, "
                        f"ADX={p['adx_mean']:.2f}, vol={p['volatility_score']:.4f})"
                    )
                else:
                    mapping[p['state']] = f"State_{p['state']}"
                    logger.info(f"State {p['state']} -> State_{p['state']} (no available names)")
        
        # ç¡®ä¿æ‰€æœ‰çŠ¶æ€éƒ½æœ‰æ˜ å°„
        for state in range(self.n_states):
            if state not in mapping:
                mapping[state] = f"State_{state}"
        
        # æ˜ å°„åˆç†æ€§æ£€æŸ¥ï¼šéªŒè¯åˆ†é…çš„åç§°æ˜¯å¦ä¸ç‰¹å¾ä¸€è‡´
        self._validate_mapping(mapping, profiles, adx_median, vol_median)
        
        # ä¿å­˜æ˜ å°„å’Œ profiles
        self.regime_mapping_ = mapping
        self.state_profiles_ = profiles  # ä¿å­˜ç‰¹å¾ profileï¼ˆç”¨äºå®¡è®¡ï¼‰
        
        logger.info(f"è‡ªåŠ¨æ˜ å°„å®Œæˆ: {mapping}")
        return mapping
    
    def _validate_mapping(
        self, 
        mapping: Dict[int, str], 
        profiles: List[Dict],
        adx_median: float,
        vol_median: float
    ):
        """
        éªŒè¯æ˜ å°„ç»“æœæ˜¯å¦åˆç†
        
        æ£€æŸ¥åˆ†é…çš„è¯­ä¹‰åç§°æ˜¯å¦ä¸çŠ¶æ€ç‰¹å¾ä¸€è‡´ï¼Œä¸ä¸€è‡´æ—¶è®°å½•è­¦å‘Šã€‚
        """
        profile_dict = {p['state']: p for p in profiles}
        
        for state, name in mapping.items():
            if state not in profile_dict:
                continue
            p = profile_dict[state]
            
            # Strong_Trend åº”è¯¥æœ‰è¾ƒé«˜çš„ ADX
            if name == 'Strong_Trend' and p['adx_mean'] < adx_median * 0.8:
                logger.warning(
                    f"âš ï¸ æ˜ å°„å¯èƒ½ä¸åˆç†: State {state} è¢«æ˜ å°„ä¸º {name}ï¼Œ"
                    f"ä½† ADX={p['adx_mean']:.2f} ä½äºä¸­ä½æ•°*0.8={adx_median*0.8:.2f}"
                )
            
            # Squeeze åº”è¯¥æœ‰è¾ƒä½çš„æ³¢åŠ¨ç‡
            if name == 'Squeeze' and p['volatility_score'] > vol_median * 1.5:
                logger.warning(
                    f"âš ï¸ æ˜ å°„å¯èƒ½ä¸åˆç†: State {state} è¢«æ˜ å°„ä¸º {name}ï¼Œ"
                    f"ä½†æ³¢åŠ¨ç‡={p['volatility_score']:.4f} é«˜äºä¸­ä½æ•°*1.5={vol_median*1.5:.4f}"
                )
            
            # Volatility_Spike åº”è¯¥æœ‰è¾ƒé«˜çš„æ³¢åŠ¨ç‡
            if name == 'Volatility_Spike' and p['volatility_score'] < vol_median * 0.8:
                logger.warning(
                    f"âš ï¸ æ˜ å°„å¯èƒ½ä¸åˆç†: State {state} è¢«æ˜ å°„ä¸º {name}ï¼Œ"
                    f"ä½†æ³¢åŠ¨ç‡={p['volatility_score']:.4f} ä½äºä¸­ä½æ•°*0.8={vol_median*0.8:.4f}"
                )
    
    def get_regime_name(self, state_id: int) -> str:
        """
        è·å–çŠ¶æ€ ID å¯¹åº”çš„è¯­ä¹‰åç§°
        
        Args:
            state_id: çŠ¶æ€ ID
            
        Returns:
            è¯­ä¹‰åç§°
        """
        if self.regime_mapping_ is None:
            return f"State_{state_id}"
        return self.regime_mapping_.get(state_id, f"State_{state_id}")
    
    def get_regime_mapping(self) -> Dict[int, str]:
        """
        è·å–å®Œæ•´çš„çŠ¶æ€æ˜ å°„
        
        Returns:
            {state_id: regime_name} æ˜ å°„å­—å…¸
        """
        if self.regime_mapping_ is None:
            return {i: f"State_{i}" for i in range(self.n_states)}
        return self.regime_mapping_.copy()
    
    # ==================== BIC éªŒè¯åŠŸèƒ½ ====================
    
    def validate_n_states(
        self, 
        features: pd.DataFrame, 
        n_states_range: List[int] = None,
        n_iter: int = 100
    ) -> Dict:
        """
        ä½¿ç”¨ BIC éªŒè¯çŠ¶æ€æ•°é‡æ˜¯å¦åˆç†
        
        Args:
            features: ç‰¹å¾ DataFrame
            n_states_range: è¦æµ‹è¯•çš„çŠ¶æ€æ•°é‡èŒƒå›´ï¼Œé»˜è®¤ [4, 5, 6, 7, 8]
            n_iter: HMM è®­ç»ƒè¿­ä»£æ¬¡æ•°
            
        Returns:
            åŒ…å«å„çŠ¶æ€æ•°é‡ BIC å€¼çš„å­—å…¸
        """
        if n_states_range is None:
            n_states_range = [4, 5, 6, 7, 8]
        
        logger.info(f"å¼€å§‹ BIC éªŒè¯ï¼Œæµ‹è¯•çŠ¶æ€æ•°é‡: {n_states_range}")
        
        # é¢„å¤„ç†æ•°æ®
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        
        pca = PCA(n_components=self.n_components)
        features_pca = pca.fit_transform(features_scaled)
        
        results = {}
        best_n_states = None
        best_bic = float('inf')
        
        for n_states in n_states_range:
            try:
                model = hmm.GaussianHMM(
                    n_components=n_states,
                    covariance_type="full",
                    n_iter=n_iter,
                    random_state=42
                )
                model.fit(features_pca)
                bic = model.bic(features_pca)
                results[n_states] = {
                    'bic': bic,
                    'converged': model.monitor_.converged
                }
                
                if bic < best_bic:
                    best_bic = bic
                    best_n_states = n_states
                
                logger.info(f"  n_states={n_states}: BIC={bic:.2f}, converged={model.monitor_.converged}")
                
            except Exception as e:
                logger.warning(f"  n_states={n_states}: è®­ç»ƒå¤±è´¥ - {e}")
                results[n_states] = {'bic': None, 'error': str(e)}
        
        results['best_n_states'] = best_n_states
        results['best_bic'] = best_bic
        results['current_n_states'] = self.n_states
        results['recommendation'] = (
            f"å»ºè®®ä½¿ç”¨ {best_n_states} ä¸ªçŠ¶æ€ï¼ˆBIC={best_bic:.2f}ï¼‰" 
            if best_n_states != self.n_states 
            else f"å½“å‰ {self.n_states} ä¸ªçŠ¶æ€æ˜¯æœ€ä¼˜é€‰æ‹©"
        )
        
        logger.info(f"BIC éªŒè¯å®Œæˆ: {results['recommendation']}")
        return results
    
    # ==================== è½¬ç§»çŸ©é˜µåˆ†æ ====================
    
    def compute_transition_matrix(self, states: np.ndarray) -> np.ndarray:
        """
        è®¡ç®—çŠ¶æ€è½¬ç§»çŸ©é˜µï¼ˆç»éªŒä¼°è®¡ï¼‰
        
        Args:
            states: çŠ¶æ€åºåˆ—
            
        Returns:
            è½¬ç§»çŸ©é˜µ (n_states x n_states)
        """
        transition_counts = np.zeros((self.n_states, self.n_states))
        
        for i in range(len(states) - 1):
            from_state = states[i]
            to_state = states[i + 1]
            transition_counts[from_state, to_state] += 1
        
        # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡
        row_sums = transition_counts.sum(axis=1, keepdims=True)
        row_sums[row_sums == 0] = 1  # é¿å…é™¤ä»¥é›¶
        transition_matrix = transition_counts / row_sums
        
        self.transition_matrix_ = transition_matrix
        return transition_matrix
    
    def compute_dwell_times(self, states: np.ndarray) -> Dict[int, Dict]:
        """
        è®¡ç®—æ¯ä¸ªçŠ¶æ€çš„é©»ç•™æ—¶é—´åˆ†å¸ƒ
        
        Args:
            states: çŠ¶æ€åºåˆ—
            
        Returns:
            æ¯ä¸ªçŠ¶æ€çš„é©»ç•™æ—¶é—´ç»Ÿè®¡
        """
        dwell_times = {i: [] for i in range(self.n_states)}
        
        if len(states) == 0:
            return {i: {'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'count': 0} 
                    for i in range(self.n_states)}
        
        current_state = states[0]
        current_dwell = 1
        
        for i in range(1, len(states)):
            if states[i] == current_state:
                current_dwell += 1
            else:
                dwell_times[current_state].append(current_dwell)
                current_state = states[i]
                current_dwell = 1
        
        # è®°å½•æœ€åä¸€ä¸ªçŠ¶æ€çš„é©»ç•™æ—¶é—´
        dwell_times[current_state].append(current_dwell)
        
        # è®¡ç®—ç»Ÿè®¡é‡
        result = {}
        for state, times in dwell_times.items():
            if times:
                result[state] = {
                    'mean': np.mean(times),
                    'std': np.std(times),
                    'min': np.min(times),
                    'max': np.max(times),
                    'count': len(times)
                }
            else:
                result[state] = {'mean': 0, 'std': 0, 'min': 0, 'max': 0, 'count': 0}
        
        return result
    
    def analyze_regime_stability(
        self, 
        states: np.ndarray, 
        switch_threshold: int = 10
    ) -> Dict:
        """
        åˆ†æ regime ç¨³å®šæ€§ï¼ˆæ£€æµ‹å¼‚å¸¸é¢‘ç¹åˆ‡æ¢ï¼‰
        
        Args:
            states: çŠ¶æ€åºåˆ—
            switch_threshold: æ¯å°æ—¶åˆ‡æ¢æ¬¡æ•°è­¦å‘Šé˜ˆå€¼ï¼ˆå‡è®¾æ•°æ®æ˜¯15åˆ†é’Ÿé¢‘ç‡ï¼‰
            
        Returns:
            ç¨³å®šæ€§åˆ†æç»“æœ
        """
        if len(states) < 2:
            return {'switches': 0, 'switch_rate': 0, 'warning': False}
        
        # è®¡ç®—çŠ¶æ€åˆ‡æ¢æ¬¡æ•°
        switches = np.sum(states[1:] != states[:-1])
        
        # å‡è®¾æ•°æ®æ˜¯15åˆ†é’Ÿé¢‘ç‡ï¼Œè®¡ç®—æ¯å°æ—¶åˆ‡æ¢æ¬¡æ•°
        # æ¯å°æ—¶æœ‰ 4 ä¸ª15åˆ†é’Ÿå‘¨æœŸ
        hours = len(states) / 4
        switch_rate = switches / hours if hours > 0 else 0
        
        # æ£€æŸ¥æ˜¯å¦å¼‚å¸¸é¢‘ç¹åˆ‡æ¢
        warning = switch_rate > switch_threshold
        
        result = {
            'total_switches': int(switches),
            'switch_rate_per_hour': switch_rate,
            'warning': warning,
            'message': (
                f"âš ï¸ å¼‚å¸¸é¢‘ç¹åˆ‡æ¢: {switch_rate:.1f} æ¬¡/å°æ—¶ > {switch_threshold}" 
                if warning 
                else f"âœ“ æ­£å¸¸: {switch_rate:.1f} æ¬¡/å°æ—¶"
            )
        }
        
        if warning:
            logger.warning(result['message'])
        else:
            logger.info(result['message'])
        
        return result
    
    # ==================== çŠ¶æ€åˆ†å¸ƒæ£€æŸ¥åŠŸèƒ½ ====================
    
    def check_state_distribution(
        self,
        train_states: np.ndarray,
        val_states: Optional[np.ndarray],
        test_states: Optional[np.ndarray],
        min_samples_per_state: int = 10,
        min_ratio_per_state: float = 0.01
    ) -> Dict:
        """
        æ£€æŸ¥å„æ•°æ®é›†çš„çŠ¶æ€åˆ†å¸ƒæ˜¯å¦å¥åº·
        
        æ£€æµ‹ä»¥ä¸‹é—®é¢˜ï¼š
        1. æŸçŠ¶æ€åœ¨éªŒè¯é›†/æµ‹è¯•é›†ä¸­å®Œå…¨ç¼ºå¤±ï¼ˆæ ·æœ¬æ•°ä¸º 0ï¼‰
        2. æŸçŠ¶æ€åœ¨éªŒè¯é›†/æµ‹è¯•é›†ä¸­æ ·æœ¬è¿‡å°‘ï¼ˆä½äºé˜ˆå€¼ï¼‰
        3. è®­ç»ƒé›†å’ŒéªŒè¯é›†/æµ‹è¯•é›†çš„åˆ†å¸ƒå·®å¼‚è¿‡å¤§
        
        Args:
            train_states: è®­ç»ƒé›†çŠ¶æ€
            val_states: éªŒè¯é›†çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            test_states: æµ‹è¯•é›†çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
            min_samples_per_state: æ¯ä¸ªçŠ¶æ€çš„æœ€å°æ ·æœ¬æ•°
            min_ratio_per_state: æ¯ä¸ªçŠ¶æ€çš„æœ€å°å æ¯”
            
        Returns:
            æ£€æŸ¥ç»“æœï¼ŒåŒ…å« warnings å’Œ recommendations
        """
        result = {
            'healthy': True,
            'warnings': [],
            'missing_states': {
                'val': [],
                'test': []
            },
            'low_sample_states': {
                'val': [],
                'test': []
            },
            'distributions': {},
            'recommendations': []
        }
        
        # è®¡ç®—è®­ç»ƒé›†åˆ†å¸ƒ
        train_dist = np.bincount(train_states, minlength=self.n_states)
        train_total = len(train_states)
        train_ratios = train_dist / train_total
        result['distributions']['train'] = {
            'counts': train_dist.tolist(),
            'ratios': train_ratios.tolist()
        }
        
        # æ£€æŸ¥éªŒè¯é›†
        if val_states is not None and len(val_states) > 0:
            val_dist = np.bincount(val_states, minlength=self.n_states)
            val_total = len(val_states)
            val_ratios = val_dist / val_total
            result['distributions']['val'] = {
                'counts': val_dist.tolist(),
                'ratios': val_ratios.tolist()
            }
            
            # æ£€æŸ¥ç¼ºå¤±çŠ¶æ€
            for state in range(self.n_states):
                state_name = self.get_regime_name(state)
                
                if val_dist[state] == 0:
                    result['healthy'] = False
                    result['missing_states']['val'].append(state)
                    warning = (
                        f"âš ï¸ çŠ¶æ€ {state} ({state_name}) åœ¨éªŒè¯é›†ä¸­å®Œå…¨ç¼ºå¤±ï¼"
                        f"è®­ç»ƒé›†æœ‰ {train_dist[state]} ä¸ªæ ·æœ¬ ({train_ratios[state]:.1%})"
                    )
                    result['warnings'].append(warning)
                    logger.warning(warning)
                    
                elif val_dist[state] < min_samples_per_state:
                    result['low_sample_states']['val'].append(state)
                    warning = (
                        f"âš ï¸ çŠ¶æ€ {state} ({state_name}) åœ¨éªŒè¯é›†ä¸­æ ·æœ¬è¿‡å°‘ï¼š"
                        f"{val_dist[state]} ä¸ª (< {min_samples_per_state})"
                    )
                    result['warnings'].append(warning)
                    logger.warning(warning)
                    
                elif val_ratios[state] < min_ratio_per_state:
                    result['low_sample_states']['val'].append(state)
                    warning = (
                        f"âš ï¸ çŠ¶æ€ {state} ({state_name}) åœ¨éªŒè¯é›†ä¸­å æ¯”è¿‡ä½ï¼š"
                        f"{val_ratios[state]:.2%} (< {min_ratio_per_state:.1%})"
                    )
                    result['warnings'].append(warning)
                    logger.warning(warning)
        
        # æ£€æŸ¥æµ‹è¯•é›†
        if test_states is not None and len(test_states) > 0:
            test_dist = np.bincount(test_states, minlength=self.n_states)
            test_total = len(test_states)
            test_ratios = test_dist / test_total
            result['distributions']['test'] = {
                'counts': test_dist.tolist(),
                'ratios': test_ratios.tolist()
            }
            
            # æ£€æŸ¥ç¼ºå¤±çŠ¶æ€
            for state in range(self.n_states):
                state_name = self.get_regime_name(state)
                
                if test_dist[state] == 0:
                    result['missing_states']['test'].append(state)
                    warning = (
                        f"âš ï¸ çŠ¶æ€ {state} ({state_name}) åœ¨æµ‹è¯•é›†ä¸­å®Œå…¨ç¼ºå¤±ï¼"
                        f"è®­ç»ƒé›†æœ‰ {train_dist[state]} ä¸ªæ ·æœ¬ ({train_ratios[state]:.1%})"
                    )
                    result['warnings'].append(warning)
                    logger.warning(warning)
        
        # ç”Ÿæˆå»ºè®®
        if result['missing_states']['val']:
            result['recommendations'].append(
                "éªŒè¯é›†ç¼ºå¤±æŸäº›çŠ¶æ€ï¼Œè¿™æ˜¯æ—¶é—´åºåˆ—æŒ‰æ—¶é—´åˆ’åˆ†çš„æ­£å¸¸ç°è±¡ã€‚"
                "HMM åœ¨è®­ç»ƒé›†ä¸Š fit æ—¶ä¼šè‡ªåŠ¨å‘ç° 6 ä¸ªçŠ¶æ€ï¼Œä½†éªŒè¯/æµ‹è¯•æ—¶é—´æ®µå†…"
                "å¯èƒ½æ²¡æœ‰å‡ºç°æŸäº›å¸‚åœºçŠ¶æ€ï¼ˆå¦‚ Volatility Spike æˆ– Squeezeï¼‰ã€‚"
                "å¯ä»¥å°è¯•ï¼š1) å¢åŠ æ•°æ®å¤©æ•°ï¼›2) è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹ï¼›3) æ¥å—è¿™æ˜¯çœŸå®å¸‚åœºçš„æƒ…å†µã€‚"
            )
        
        if result['low_sample_states']['val']:
            result['recommendations'].append(
                "å»ºè®®ï¼šéªŒè¯é›†æŸäº›çŠ¶æ€æ ·æœ¬è¿‡å°‘ï¼Œearly stopping å¯èƒ½æ— æ³•å‡†ç¡®è¯„ä¼°è¿™äº›çŠ¶æ€ã€‚"
                "è€ƒè™‘å¢å¤§éªŒè¯é›†æ¯”ä¾‹ï¼ˆå¦‚ä» 15% å¢åŠ åˆ° 20%ï¼‰ã€‚"
            )
        
        # æ‰“å°æŒ‰è¯­ä¹‰åç§°æ’åºçš„åˆ†å¸ƒï¼ˆä¾¿äºè·¨è®­ç»ƒæ¯”è¾ƒï¼‰
        self._print_distribution_by_regime_name(
            train_states, val_states, test_states
        )
        
        # æ‰“å°æ€»ç»“
        if result['healthy']:
            logger.info("âœ“ çŠ¶æ€åˆ†å¸ƒæ£€æŸ¥é€šè¿‡ï¼šæ‰€æœ‰çŠ¶æ€åœ¨å„æ•°æ®é›†ä¸­éƒ½æœ‰è¶³å¤Ÿæ ·æœ¬")
        else:
            missing_val = len(result['missing_states']['val'])
            missing_test = len(result['missing_states']['test'])
            logger.warning(
                f"çŠ¶æ€åˆ†å¸ƒæ£€æŸ¥å‘ç°é—®é¢˜ï¼šéªŒè¯é›†ç¼ºå¤± {missing_val} ä¸ªçŠ¶æ€ï¼Œæµ‹è¯•é›†ç¼ºå¤± {missing_test} ä¸ªçŠ¶æ€"
            )
            logger.info(
                "  ğŸ“ è§£é‡Šï¼šè¿™æ˜¯æ—¶é—´åºåˆ—æŒ‰æ—¶é—´åˆ’åˆ†çš„æ­£å¸¸ç°è±¡ã€‚"
                "HMM åœ¨è®­ç»ƒé›†ä¸Š fit æ—¶ä¼šè‡ªåŠ¨èšç±»å‡º 6 ä¸ªçŠ¶æ€ï¼Œä½†éªŒè¯/æµ‹è¯•æ—¶é—´æ®µå†…"
                "å¯èƒ½æ²¡æœ‰å‡ºç°æŸäº›å¸‚åœºçŠ¶æ€ï¼ˆå¦‚æç«¯æ³¢åŠ¨æˆ–æä½æ³¢åŠ¨æœŸï¼‰ã€‚"
            )
            for rec in result['recommendations']:
                logger.info(f"  ğŸ’¡ {rec}")
        
        return result
    
    def _print_distribution_by_regime_name(
        self,
        train_states: np.ndarray,
        val_states: Optional[np.ndarray],
        test_states: Optional[np.ndarray]
    ):
        """
        æŒ‰è¯­ä¹‰åç§°é¡ºåºæ‰“å°çŠ¶æ€åˆ†å¸ƒï¼ˆä¾¿äºè·¨è®­ç»ƒæ¯”è¾ƒï¼‰
        
        è¯­ä¹‰åç§°é¡ºåºå›ºå®šä¸ºï¼š
        Choppy_High_Vol, Strong_Trend, Volatility_Spike, Weak_Trend, Range, Squeeze
        
        è¿™æ ·æ— è®º HMM çŠ¶æ€ç¼–å·å¦‚ä½•å˜åŒ–ï¼Œç›¸åŒè¯­ä¹‰çš„çŠ¶æ€æ€»æ˜¯åœ¨åŒä¸€ä½ç½®æ˜¾ç¤ºã€‚
        
        æ³¨æ„ï¼šå¦‚æœ regime_mapping_ æœªåˆå§‹åŒ–ï¼ˆå¦‚åœ¨ auto_optimize_n_states ä¸­ï¼‰ï¼Œ
        æ­¤æ–¹æ³•ä¼šè·³è¿‡æ‰“å°ï¼Œé¿å…è¾“å‡ºå…¨ 0 çš„è¯¯å¯¼æ€§ä¿¡æ¯ã€‚
        """
        # å¦‚æœæ²¡æœ‰ regime_mappingï¼Œè·³è¿‡æŒ‰è¯­ä¹‰åç§°æ‰“å°ï¼ˆé¿å…å…¨ 0 è¾“å‡ºï¼‰
        if self.regime_mapping_ is None:
            logger.debug("è·³è¿‡æŒ‰è¯­ä¹‰åç§°æ‰“å°ï¼ˆregime_mapping æœªåˆå§‹åŒ–ï¼‰")
            return
        
        # å®šä¹‰è¯­ä¹‰åç§°çš„å›ºå®šé¡ºåºï¼ˆä¸ config.py REGIME_NAMES ä¸€è‡´ï¼‰
        SEMANTIC_ORDER = [
            "Choppy_High_Vol",   # é«˜æ³¢åŠ¨æ— æ–¹å‘
            "Strong_Trend",      # å¼ºè¶‹åŠ¿
            "Volatility_Spike",  # æ³¢åŠ¨ç‡çªå¢
            "Weak_Trend",        # å¼±è¶‹åŠ¿
            "Range",             # åŒºé—´éœ‡è¡
            "Squeeze"            # ä½æ³¢åŠ¨è“„åŠ¿
        ]
        
        # æ„å»ºè¯­ä¹‰åç§°åˆ°çŠ¶æ€ç¼–å·çš„åå‘æ˜ å°„
        # æ³¨æ„ï¼šåˆ°è¾¾è¿™é‡Œæ—¶ regime_mapping_ ä¸€å®šä¸æ˜¯ Noneï¼ˆå·²åœ¨ä¸Šé¢æ£€æŸ¥ï¼‰
        name_to_state = {name: state for state, name in self.regime_mapping_.items()}
        
        # è®¡ç®—å„æ•°æ®é›†çš„åˆ†å¸ƒ
        train_dist = np.bincount(train_states, minlength=self.n_states)
        val_dist = np.bincount(val_states, minlength=self.n_states) if val_states is not None else None
        test_dist = np.bincount(test_states, minlength=self.n_states) if test_states is not None else None
        
        # æŒ‰è¯­ä¹‰åç§°é¡ºåºæ„å»ºåˆ†å¸ƒï¼ˆè½¬æ¢ä¸º Python intï¼Œé¿å…æ‰“å° np.int64ï¼‰
        train_by_name = []
        val_by_name = []
        test_by_name = []
        
        for name in SEMANTIC_ORDER:
            state = name_to_state.get(name)
            if state is not None and state < len(train_dist):
                train_by_name.append(int(train_dist[state]))
                if val_dist is not None:
                    val_by_name.append(int(val_dist[state]))
                if test_dist is not None:
                    test_by_name.append(int(test_dist[state]))
            else:
                train_by_name.append(0)
                if val_dist is not None:
                    val_by_name.append(0)
                if test_dist is not None:
                    test_by_name.append(0)
        
        # æ‰“å°æŒ‰è¯­ä¹‰åç§°æ’åºçš„åˆ†å¸ƒ
        logger.info("=" * 70)
        logger.info("çŠ¶æ€åˆ†å¸ƒï¼ˆæŒ‰è¯­ä¹‰åç§°é¡ºåºï¼Œä¾¿äºè·¨è®­ç»ƒæ¯”è¾ƒï¼‰:")
        logger.info(f"  è¯­ä¹‰åç§°é¡ºåº: {SEMANTIC_ORDER}")
        logger.info(f"  è®­ç»ƒé›†åˆ†å¸ƒ:   {train_by_name}")
        if val_dist is not None:
            logger.info(f"  éªŒè¯é›†åˆ†å¸ƒ:   {val_by_name}")
        if test_dist is not None:
            logger.info(f"  æµ‹è¯•é›†åˆ†å¸ƒ:   {test_by_name}")
        logger.info("=" * 70)
    
    # ==================== æ˜ å°„æ¯”å¯¹åŠŸèƒ½ ====================
    
    def compare_mapping(self, old_mapping: Dict[int, str], threshold: int = 2) -> Dict:
        """
        æ¯”è¾ƒæ–°æ—§æ˜ å°„çš„å·®å¼‚ï¼ˆåŸºäºè¯­ä¹‰åç§°é›†åˆï¼Œè€Œé state idï¼‰
        
        æ³¨æ„ï¼šHMM çŠ¶æ€ç¼–å·æ˜¯ä»»æ„çš„ï¼Œä¸¤æ¬¡è®­ç»ƒå³ä½¿å‘ç°ç›¸åŒçš„å¸‚åœºçŠ¶æ€ï¼Œ
        ç¼–å·ä¹Ÿå¯èƒ½ä¸åŒã€‚å› æ­¤æˆ‘ä»¬æ¯”è¾ƒ**è¯­ä¹‰åç§°é›†åˆ**è€ŒéæŒ‰ state id æ¯”è¾ƒã€‚
        
        ä¾‹å¦‚ï¼š
        - æ—§æ¨¡å‹ï¼š{0: Strong_Trend, 1: Range}
        - æ–°æ¨¡å‹ï¼š{0: Range, 1: Strong_Trend}
        è¿™ä¸¤ä¸ªæ˜ å°„çš„è¯­ä¹‰æ˜¯ä¸€è‡´çš„ï¼Œä¸åº”è¯¥æŠ¥å‘Šå·®å¼‚ã€‚
        
        Args:
            old_mapping: æ—§çš„çŠ¶æ€æ˜ å°„
            threshold: å…è®¸çš„è¯­ä¹‰åç§°å·®å¼‚æ•°é‡ï¼ˆæ–°å¢æˆ–æ¶ˆå¤±çš„åç§°ï¼‰
            
        Returns:
            æ¯”å¯¹ç»“æœ
        """
        if self.regime_mapping_ is None:
            return {
                'identical': False,
                'semantic_diff_count': -1,
                'message': "å½“å‰æ¨¡å‹æ²¡æœ‰çŠ¶æ€æ˜ å°„",
                'needs_review': True
            }
        
        new_mapping = self.regime_mapping_
        
        # æå–è¯­ä¹‰åç§°é›†åˆï¼ˆå¿½ç•¥ state idï¼‰
        old_names = set(old_mapping.values())
        new_names = set(new_mapping.values())
        
        # è®¡ç®—è¯­ä¹‰å·®å¼‚
        names_added = new_names - old_names  # æ–°å¢çš„åç§°
        names_removed = old_names - new_names  # æ¶ˆå¤±çš„åç§°
        names_unchanged = old_names & new_names  # ä¿æŒä¸å˜çš„åç§°
        
        semantic_diff_count = len(names_added) + len(names_removed)
        semantic_identical = semantic_diff_count == 0
        needs_review = semantic_diff_count > threshold
        
        # åŒæ—¶è®°å½• state id çº§åˆ«çš„å˜åŒ–ï¼ˆä»…ä¾›å‚è€ƒï¼‰
        state_id_changes = []
        for state in range(self.n_states):
            old_name = old_mapping.get(state, f"State_{state}")
            new_name = new_mapping.get(state, f"State_{state}")
            if old_name != new_name:
                state_id_changes.append({
                    'state': state,
                    'old': old_name,
                    'new': new_name
                })
        
        result = {
            'semantic_identical': semantic_identical,
            'semantic_diff_count': semantic_diff_count,
            'names_added': list(names_added),
            'names_removed': list(names_removed),
            'names_unchanged': list(names_unchanged),
            'state_id_changes': state_id_changes,  # ä»…ä¾›å‚è€ƒï¼Œä¸ç”¨äºåˆ¤æ–­
            'threshold': threshold,
            'needs_review': needs_review,
            'message': self._build_comparison_message(
                semantic_identical, semantic_diff_count, 
                names_added, names_removed, state_id_changes, threshold
            )
        }
        
        if needs_review:
            logger.warning(result['message'])
        else:
            logger.info(result['message'])
        
        return result
    
    def _build_comparison_message(
        self, 
        semantic_identical: bool,
        semantic_diff_count: int,
        names_added: set,
        names_removed: set,
        state_id_changes: List[Dict],
        threshold: int
    ) -> str:
        """æ„å»ºæ˜ å°„æ¯”å¯¹çš„æ¶ˆæ¯"""
        if semantic_identical:
            if state_id_changes:
                return (
                    f"âœ“ è¯­ä¹‰ä¸€è‡´ï¼ˆstate id æœ‰ {len(state_id_changes)} å¤„é‡æ’ï¼Œ"
                    f"è¿™æ˜¯ HMM æ­£å¸¸è¡Œä¸ºï¼Œä¸å½±å“è¯­ä¹‰ï¼‰"
                )
            return "âœ“ æ˜ å°„å®Œå…¨ä¸€è‡´"
        
        parts = []
        if names_added:
            parts.append(f"æ–°å¢: {names_added}")
        if names_removed:
            parts.append(f"æ¶ˆå¤±: {names_removed}")
        
        if semantic_diff_count > threshold:
            return f"âš ï¸ è¯­ä¹‰å˜åŒ–è¾ƒå¤§ï¼ˆ{', '.join(parts)}ï¼‰ï¼Œå»ºè®®äººå·¥å¤æ ¸"
        else:
            return f"è¯­ä¹‰æœ‰ {semantic_diff_count} å¤„å·®å¼‚ï¼ˆ{', '.join(parts)}ï¼‰ï¼Œåœ¨å¯æ¥å—èŒƒå›´å†…"
    
    def get_state_profiles(self) -> Optional[List[Dict]]:
        """
        è·å–ä¿å­˜çš„çŠ¶æ€ç‰¹å¾ profile
        
        Returns:
            çŠ¶æ€ç‰¹å¾ profile åˆ—è¡¨
        """
        return self.state_profiles_
    
    # ==================== åŠ¨æ€çŠ¶æ€æ•°é‡ä¼˜åŒ– ====================
    
    def auto_optimize_n_states(
        self,
        train_features: pd.DataFrame,
        val_features: pd.DataFrame,
        test_features: Optional[pd.DataFrame] = None,
        n_states_min: int = 4,
        n_states_max: int = 8,
        max_missing_allowed: int = 1,
        max_low_ratio_allowed: int = 2,
        strategy: str = "decrease_first",
        min_samples_per_state: int = 10,
        min_ratio_per_state: float = 0.01,
        n_iter: int = 100
    ) -> Dict:
        """
        è‡ªåŠ¨ä¼˜åŒ–çŠ¶æ€æ•°é‡ï¼Œç¡®ä¿éªŒè¯/æµ‹è¯•é›†åˆ†å¸ƒå¥åº·
        
        å½“éªŒè¯/æµ‹è¯•é›†ä¸­æŸäº›çŠ¶æ€å®Œå…¨ç¼ºå¤±æˆ–å æ¯”è¿‡ä½æ—¶ï¼Œ
        è‡ªåŠ¨å°è¯•è°ƒæ•´çŠ¶æ€æ•°é‡ï¼Œæ‰¾åˆ°ä¸€ä¸ªä½¿åˆ†å¸ƒæ›´å¥åº·çš„å€¼ã€‚
        
        Args:
            train_features: è®­ç»ƒé›†ç‰¹å¾
            val_features: éªŒè¯é›†ç‰¹å¾
            test_features: æµ‹è¯•é›†ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            n_states_min: æœ€å°çŠ¶æ€æ•°é‡
            n_states_max: æœ€å¤§çŠ¶æ€æ•°é‡
            max_missing_allowed: å…è®¸çš„æœ€å¤§ç¼ºå¤±çŠ¶æ€æ•°é‡
            max_low_ratio_allowed: å…è®¸çš„æœ€å¤§ä½å æ¯”çŠ¶æ€æ•°é‡
            strategy: è°ƒæ•´ç­–ç•¥
                - "decrease_first": ä¼˜å…ˆå‡å°‘çŠ¶æ€æ•°é‡
                - "bic_optimal": ä½¿ç”¨ BIC é€‰æ‹©æœ€ä¼˜æ•°é‡
            min_samples_per_state: åˆ¤æ–­"æ ·æœ¬è¿‡å°‘"çš„é˜ˆå€¼
            min_ratio_per_state: åˆ¤æ–­"å æ¯”è¿‡ä½"çš„é˜ˆå€¼
            n_iter: HMM è®­ç»ƒè¿­ä»£æ¬¡æ•°
            
        Returns:
            ä¼˜åŒ–ç»“æœï¼ŒåŒ…å«æœ€ä¼˜ n_statesã€å„å°è¯•çš„ç»“æœç­‰
        """
        logger.info("=" * 70)
        logger.info("å¼€å§‹è‡ªåŠ¨ä¼˜åŒ–çŠ¶æ€æ•°é‡...")
        logger.info(f"  ç­–ç•¥: {strategy}")
        logger.info(f"  çŠ¶æ€æ•°é‡èŒƒå›´: {n_states_min} - {n_states_max}")
        logger.info(f"  å…è®¸ç¼ºå¤±çŠ¶æ€æ•°: {max_missing_allowed}")
        logger.info("=" * 70)
        
        original_n_states = self.n_states
        results = {}
        best_n_states = None
        best_score = float('-inf')
        
        # ç¡®å®šå°è¯•çš„çŠ¶æ€æ•°é‡é¡ºåº
        if strategy == "decrease_first":
            # ä»å½“å‰å€¼å¼€å§‹ï¼Œä¼˜å…ˆå‘ä¸‹å°è¯•
            n_states_to_try = []
            for n in range(original_n_states, n_states_min - 1, -1):
                n_states_to_try.append(n)
            for n in range(original_n_states + 1, n_states_max + 1):
                n_states_to_try.append(n)
        else:
            # BIC ç­–ç•¥ï¼šå°è¯•æ‰€æœ‰å¯èƒ½çš„å€¼
            n_states_to_try = list(range(n_states_min, n_states_max + 1))
        
        for n_states in n_states_to_try:
            logger.info(f"\nå°è¯• n_states = {n_states}...")
            
            try:
                # åˆ›å»ºä¸´æ—¶ HMM å®ä¾‹
                temp_hmm = HMMRegimeLabeler(
                    n_states=n_states,
                    n_components=self.n_components,
                    primary_timeframe=self.primary_timeframe
                )
                
                # è®­ç»ƒå¹¶é¢„æµ‹
                train_states, val_states, test_states = temp_hmm.fit_predict_split(
                    train_features, val_features, test_features, n_iter=n_iter
                )
                
                # æ£€æŸ¥åˆ†å¸ƒå¥åº·åº¦
                dist_check = temp_hmm.check_state_distribution(
                    train_states, val_states, test_states,
                    min_samples_per_state, min_ratio_per_state
                )
                
                # è®¡ç®—è¯„åˆ†
                missing_val = len(dist_check['missing_states']['val'])
                missing_test = len(dist_check['missing_states']['test'])
                low_ratio_val = len(dist_check['low_sample_states']['val'])
                
                # å¥åº·åº¦è¯„åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰
                # - æ¯ä¸ªç¼ºå¤±çŠ¶æ€æ‰£ 10 åˆ†
                # - æ¯ä¸ªä½å æ¯”çŠ¶æ€æ‰£ 3 åˆ†
                # - BIC è¶Šä½åŠ åˆ†ï¼ˆå½’ä¸€åŒ–åˆ° 0-5 åˆ†ï¼‰
                health_score = 100 - (missing_val * 10) - (missing_test * 5) - (low_ratio_val * 3)
                
                # BIC è¯„åˆ†ï¼ˆéœ€è¦åœ¨ç›¸åŒæ•°æ®ä¸Šæ¯”è¾ƒæ‰æœ‰æ„ä¹‰ï¼‰
                bic = temp_hmm.training_bic_ if temp_hmm.training_bic_ else float('inf')
                
                result = {
                    'n_states': n_states,
                    'bic': bic,
                    'missing_val': missing_val,
                    'missing_test': missing_test,
                    'low_ratio_val': low_ratio_val,
                    'health_score': health_score,
                    'is_healthy': dist_check['healthy'],
                    'train_dist': dist_check['distributions']['train']['counts'],
                    'val_dist': dist_check['distributions'].get('val', {}).get('counts', []),
                }
                
                results[n_states] = result
                
                logger.info(f"  BIC: {bic:.2f}")
                logger.info(f"  éªŒè¯é›†ç¼ºå¤±: {missing_val}, æµ‹è¯•é›†ç¼ºå¤±: {missing_test}")
                logger.info(f"  å¥åº·è¯„åˆ†: {health_score}")
                
                # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ¡ä»¶
                if missing_val <= max_missing_allowed and low_ratio_val <= max_low_ratio_allowed:
                    if health_score > best_score:
                        best_score = health_score
                        best_n_states = n_states
                        
                        # å¦‚æœæ˜¯ decrease_first ç­–ç•¥ä¸”æ‰¾åˆ°æ»¡è¶³æ¡ä»¶çš„ï¼Œç«‹å³è¿”å›
                        if strategy == "decrease_first" and dist_check['healthy']:
                            logger.info(f"âœ“ æ‰¾åˆ°å¥åº·çš„çŠ¶æ€æ•°é‡: {n_states}")
                            break
                            
            except Exception as e:
                logger.warning(f"  n_states={n_states} è®­ç»ƒå¤±è´¥: {e}")
                results[n_states] = {'error': str(e)}
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å®Œå…¨å¥åº·çš„é…ç½®ï¼Œé€‰æ‹©æœ€ä½³çš„
        if best_n_states is None:
            # é€‰æ‹©ç¼ºå¤±æœ€å°‘çš„
            valid_results = {k: v for k, v in results.items() if 'error' not in v}
            if valid_results:
                best_n_states = min(
                    valid_results.keys(),
                    key=lambda k: (valid_results[k]['missing_val'], valid_results[k]['missing_test'])
                )
                logger.warning(f"æœªæ‰¾åˆ°å®Œå…¨å¥åº·çš„é…ç½®ï¼Œé€‰æ‹©æœ€ä½³: n_states={best_n_states}")
            else:
                best_n_states = original_n_states
                logger.warning(f"æ‰€æœ‰é…ç½®éƒ½å¤±è´¥ï¼Œä¿æŒåŸå€¼: n_states={best_n_states}")
        
        # æ›´æ–°å½“å‰å®ä¾‹çš„ n_states
        if best_n_states != original_n_states:
            logger.info(f"ğŸ”„ çŠ¶æ€æ•°é‡è°ƒæ•´: {original_n_states} -> {best_n_states}")
            self.n_states = best_n_states
        else:
            logger.info(f"âœ“ ä¿æŒåŸçŠ¶æ€æ•°é‡: {best_n_states}")
        
        optimization_result = {
            'original_n_states': original_n_states,
            'optimal_n_states': best_n_states,
            'adjusted': best_n_states != original_n_states,
            'strategy': strategy,
            'all_results': results,
            'best_result': results.get(best_n_states, {}),
            'message': (
                f"çŠ¶æ€æ•°é‡ä» {original_n_states} è°ƒæ•´ä¸º {best_n_states}"
                if best_n_states != original_n_states
                else f"çŠ¶æ€æ•°é‡ {best_n_states} å·²æ˜¯æœ€ä¼˜"
            )
        }
        
        logger.info("=" * 70)
        logger.info(f"çŠ¶æ€æ•°é‡ä¼˜åŒ–å®Œæˆ: {optimization_result['message']}")
        logger.info("=" * 70)
        
        return optimization_result
    
    def retrain_with_n_states(
        self,
        n_states: int,
        train_features: pd.DataFrame,
        val_features: Optional[pd.DataFrame] = None,
        test_features: Optional[pd.DataFrame] = None,
        n_iter: int = 100
    ) -> Tuple[np.ndarray, Optional[np.ndarray], Optional[np.ndarray]]:
        """
        ä½¿ç”¨æ–°çš„çŠ¶æ€æ•°é‡é‡æ–°è®­ç»ƒ
        
        Args:
            n_states: æ–°çš„çŠ¶æ€æ•°é‡
            train_features: è®­ç»ƒé›†ç‰¹å¾
            val_features: éªŒè¯é›†ç‰¹å¾
            test_features: æµ‹è¯•é›†ç‰¹å¾
            n_iter: HMM è®­ç»ƒè¿­ä»£æ¬¡æ•°
            
        Returns:
            (train_states, val_states, test_states)
        """
        logger.info(f"ä½¿ç”¨ n_states={n_states} é‡æ–°è®­ç»ƒ...")
        
        self.n_states = n_states
        
        # é‡ç½®æ¨¡å‹
        self.hmm_model = None
        self.pca = None
        self.scaler = None
        self.regime_mapping_ = None
        self.state_profiles_ = None
        
        return self.fit_predict_split(train_features, val_features, test_features, n_iter)
